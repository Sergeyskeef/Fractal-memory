# 03. Phase 2: Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸

## ğŸ¯ Ğ¦ĞµĞ»ÑŒ

Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ FractalMemory â€” Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ñ„Ñ€Ğ°ĞºÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸ĞµĞ¹.

**Ğ’Ñ€ĞµĞ¼Ñ**: 2-3 Ğ´Ğ½Ñ  
**Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚**: Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ñ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸ĞµĞ¹ L0â†’L1â†’L2â†’L3

---

## ğŸ“‹ Ğ§ĞµĞº-Ğ»Ğ¸ÑÑ‚ Phase 2

- [ ] GraphitiAdapter ÑĞ¾Ğ·Ğ´Ğ°Ğ½
- [ ] FractalMemory ÑĞ¾Ğ·Ğ´Ğ°Ğ½
- [ ] ĞœĞµÑ‚Ğ¾Ğ´Ñ‹ remember/recall Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚
- [ ] ĞšĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚
- [ ] Soft delete Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚
- [ ] Unit Ñ‚ĞµÑÑ‚Ñ‹ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ÑÑ‚
- [ ] Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ÑÑ‚

---

## 1ï¸âƒ£ GraphitiAdapter (Ğ¾Ğ±Ñ‘Ñ€Ñ‚ĞºĞ° Ğ½Ğ°Ğ´ Graphiti)

### Ğ¤Ğ°Ğ¹Ğ» `src/core/graphiti_adapter.py`:

```python
"""
GraphitiAdapter â€” Ğ¾Ğ±Ñ‘Ñ€Ñ‚ĞºĞ° Ğ½Ğ°Ğ´ Graphiti.

ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Graphiti Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ:
1. Ğ˜Ğ·Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ (ĞµÑĞ»Ğ¸ API Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑÑ)
2. Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑĞ²Ğ¾Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ (soft delete, Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸)
3. Ğ£Ğ¿Ñ€Ğ¾Ñ‰Ğ°ĞµĞ¼ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ mock)

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:
    adapter = GraphitiAdapter(config)
    await adapter.initialize()
    await adapter.add_episode(...)
    results = await adapter.search(...)
"""

from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Optional, Any
from abc import ABC, abstractmethod
import logging
import uuid

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞœĞĞ”Ğ•Ğ›Ğ˜ Ğ”ĞĞĞĞ«Ğ¥ (Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹ Ğ¾Ñ‚ Graphiti)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class Episode:
    """Ğ­Ğ¿Ğ¸Ğ·Ğ¾Ğ´ â€” ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸"""
    id: str
    content: str
    timestamp: datetime
    source: str = "conversation"
    importance_score: float = 1.0
    level: int = 1  # 1=L1, 2=L2, 3=L3
    outcome: Optional[str] = None  # success/failure/neutral
    metadata: Dict = field(default_factory=dict)
    # Soft delete
    deleted: bool = False
    deleted_at: Optional[datetime] = None


@dataclass
class Entity:
    """Ğ¡ÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑŒ â€” Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°, Ğ¿Ñ€Ğ¾ĞµĞºÑ‚, ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚"""
    id: str
    name: str
    entity_type: str
    importance_score: float = 1.0
    access_count: int = 0
    embedding: Optional[List[float]] = None
    metadata: Dict = field(default_factory=dict)
    # Soft delete
    deleted: bool = False
    deleted_at: Optional[datetime] = None


@dataclass
class SearchResult:
    """Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ğ¾Ğ¸ÑĞºĞ°"""
    content: str
    relevance_score: float
    source: str
    timestamp: datetime
    metadata: Dict = field(default_factory=dict)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞĞ‘Ğ¡Ğ¢Ğ ĞĞšĞ¢ĞĞ«Ğ™ Ğ˜ĞĞ¢Ğ•Ğ Ğ¤Ğ•Ğ™Ğ¡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class GraphMemoryInterface(ABC):
    """
    ĞĞ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.
    ĞŸĞ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (Graphiti â†’ Ğ´Ñ€ÑƒĞ³Ğ°Ñ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ°).
    """
    
    @abstractmethod
    async def initialize(self) -> None:
        """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ"""
        pass
    
    @abstractmethod
    async def add_episode(self, episode: Episode) -> str:
        """Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´, Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ ID"""
        pass
    
    @abstractmethod
    async def search(
        self, 
        query: str, 
        limit: int = 10,
        include_deleted: bool = False
    ) -> List[SearchResult]:
        """ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸"""
        pass
    
    @abstractmethod
    async def get_by_id(self, episode_id: str) -> Optional[Episode]:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´ Ğ¿Ğ¾ ID"""
        pass
    
    @abstractmethod
    async def soft_delete(self, episode_id: str) -> bool:
        """ĞœÑĞ³ĞºĞ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ (Ğ¿Ğ¾Ğ¼ĞµÑ‚Ğ¸Ñ‚ÑŒ deleted=true)"""
        pass
    
    @abstractmethod
    async def hard_delete(self, episode_id: str) -> bool:
        """Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ GC Ğ¿Ğ¾ÑĞ»Ğµ soft delete)"""
        pass
    
    @abstractmethod
    async def update_importance(
        self, 
        episode_id: str, 
        new_importance: float
    ) -> bool:
        """ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ importance score"""
        pass
    
    @abstractmethod
    async def execute_cypher(
        self, 
        query: str, 
        params: Dict = None
    ) -> List[Dict]:
        """Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¹ Cypher Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ"""
        pass
    
    @abstractmethod
    async def close(self) -> None:
        """Ğ—Ğ°ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ"""
        pass


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GRAPHITI ADAPTER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class GraphitiAdapter(GraphMemoryInterface):
    """
    ĞĞ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ´Ğ»Ñ Graphiti.
    ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ°Ñˆ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ² Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Graphiti API.
    """
    
    def __init__(self, config: Dict):
        """
        Args:
            config: {
                "neo4j_uri": "bolt://localhost:7687",
                "neo4j_user": "neo4j",
                "neo4j_password": "password",
                "llm_client": <OpenAI client>,  # Ğ´Ğ»Ñ Graphiti
                "embedder": <Embedder>  # Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾
            }
        """
        self.config = config
        self.graphiti = None
        self._initialized = False
    
    async def initialize(self) -> None:
        """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Graphiti"""
        if self._initialized:
            return
        
        try:
            from graphiti_core import Graphiti
            from graphiti_core.llm_client import OpenAIClient
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ LLM ĞºĞ»Ğ¸ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Graphiti
            llm_client = self.config.get("llm_client")
            if llm_client is None:
                llm_client = OpenAIClient()
            
            # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Graphiti
            self.graphiti = Graphiti(
                neo4j_uri=self.config["neo4j_uri"],
                neo4j_user=self.config["neo4j_user"],
                neo4j_password=self.config["neo4j_password"],
                llm_client=llm_client
            )
            
            self._initialized = True
            logger.info("GraphitiAdapter initialized successfully")
            
        except ImportError:
            raise ImportError(
                "graphiti-core not installed. "
                "Run: pip install graphiti-core"
            )
        except Exception as e:
            logger.error(f"Failed to initialize Graphiti: {e}")
            raise
    
    async def add_episode(self, episode: Episode) -> str:
        """Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ· Graphiti"""
        self._ensure_initialized()
        
        try:
            # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Graphiti
            await self.graphiti.add_episode(
                name=episode.id,
                episode_body=episode.content,
                source_description=episode.source,
                reference_time=episode.timestamp
            )
            
            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ soft delete Ğ¿Ğ¾Ğ»Ñ Ñ‡ĞµÑ€ĞµĞ· Cypher
            # (Graphiti Ğ½Ğµ Ğ·Ğ½Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ Ğ½Ğ°ÑˆĞ¸ Ğ¿Ğ¾Ğ»Ñ)
            await self.execute_cypher(
                """
                MATCH (ep:Episode {name: $id})
                SET ep.importance_score = $importance,
                    ep.level = $level,
                    ep.outcome = $outcome,
                    ep.deleted = false
                """,
                {
                    "id": episode.id,
                    "importance": episode.importance_score,
                    "level": episode.level,
                    "outcome": episode.outcome
                }
            )
            
            logger.debug(f"Episode added: {episode.id}")
            return episode.id
            
        except Exception as e:
            logger.error(f"Failed to add episode: {e}")
            raise
    
    async def search(
        self, 
        query: str, 
        limit: int = 10,
        include_deleted: bool = False
    ) -> List[SearchResult]:
        """Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ‡ĞµÑ€ĞµĞ· Graphiti"""
        self._ensure_initialized()
        
        try:
            # ĞŸĞ¾Ğ¸ÑĞº Ñ‡ĞµÑ€ĞµĞ· Graphiti
            raw_results = await self.graphiti.search(
                query=query,
                num_results=limit * 2  # Ğ±ĞµÑ€Ñ‘Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼
            )
            
            results = []
            for r in raw_results:
                # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ soft deleted
                if not include_deleted:
                    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ deleted Ñ„Ğ»Ğ°Ğ³
                    is_deleted = getattr(r, 'deleted', False)
                    if is_deleted:
                        continue
                
                results.append(SearchResult(
                    content=getattr(r, 'fact', str(r)),
                    relevance_score=getattr(r, 'score', 1.0),
                    source=getattr(r, 'source_description', 'unknown'),
                    timestamp=getattr(r, 'created_at', datetime.now()),
                    metadata={
                        "uuid": getattr(r, 'uuid', None),
                        "episode_name": getattr(r, 'name', None)
                    }
                ))
                
                if len(results) >= limit:
                    break
            
            logger.debug(f"Search returned {len(results)} results for: {query[:50]}...")
            return results
            
        except Exception as e:
            logger.error(f"Search failed: {e}")
            return []
    
    async def get_by_id(self, episode_id: str) -> Optional[Episode]:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´ Ğ¿Ğ¾ ID"""
        self._ensure_initialized()
        
        results = await self.execute_cypher(
            """
            MATCH (ep:Episode {name: $id})
            RETURN ep
            """,
            {"id": episode_id}
        )
        
        if not results:
            return None
        
        ep = results[0]["ep"]
        return Episode(
            id=ep.get("name", episode_id),
            content=ep.get("content", ""),
            timestamp=ep.get("timestamp", datetime.now()),
            source=ep.get("source_description", "unknown"),
            importance_score=ep.get("importance_score", 1.0),
            level=ep.get("level", 1),
            outcome=ep.get("outcome"),
            deleted=ep.get("deleted", False),
            deleted_at=ep.get("deleted_at")
        )
    
    async def soft_delete(self, episode_id: str) -> bool:
        """ĞœÑĞ³ĞºĞ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ"""
        self._ensure_initialized()
        
        try:
            await self.execute_cypher(
                """
                MATCH (ep:Episode {name: $id})
                SET ep.deleted = true,
                    ep.deleted_at = datetime()
                RETURN ep
                """,
                {"id": episode_id}
            )
            logger.info(f"Soft deleted episode: {episode_id}")
            return True
            
        except Exception as e:
            logger.error(f"Soft delete failed: {e}")
            return False
    
    async def hard_delete(self, episode_id: str) -> bool:
        """
        Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ.
        âš ï¸ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ´Ğ»Ñ GC Ğ¿Ğ¾ÑĞ»Ğµ soft delete Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ°!
        """
        self._ensure_initialized()
        
        try:
            await self.execute_cypher(
                """
                MATCH (ep:Episode {name: $id})
                WHERE ep.deleted = true
                DETACH DELETE ep
                """,
                {"id": episode_id}
            )
            logger.info(f"Hard deleted episode: {episode_id}")
            return True
            
        except Exception as e:
            logger.error(f"Hard delete failed: {e}")
            return False
    
    async def update_importance(
        self, 
        episode_id: str, 
        new_importance: float
    ) -> bool:
        """ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ importance score"""
        self._ensure_initialized()
        
        try:
            await self.execute_cypher(
                """
                MATCH (ep:Episode {name: $id})
                SET ep.importance_score = $importance,
                    ep.last_accessed = datetime()
                """,
                {"id": episode_id, "importance": new_importance}
            )
            return True
            
        except Exception as e:
            logger.error(f"Update importance failed: {e}")
            return False
    
    async def execute_cypher(
        self, 
        query: str, 
        params: Dict = None
    ) -> List[Dict]:
        """Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¹ Cypher Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ"""
        self._ensure_initialized()
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ driver Ğ¸Ğ· Graphiti
        driver = self.graphiti._driver
        
        async with driver.session() as session:
            result = await session.run(query, params or {})
            return [dict(record) for record in await result.data()]
    
    async def close(self) -> None:
        """Ğ—Ğ°ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ"""
        if self.graphiti:
            await self.graphiti.close()
            self._initialized = False
            logger.info("GraphitiAdapter closed")
    
    def _ensure_initialized(self):
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ñ‡Ñ‚Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½"""
        if not self._initialized:
            raise RuntimeError(
                "GraphitiAdapter not initialized. "
                "Call await adapter.initialize() first."
            )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MOCK ADAPTER (Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MockGraphMemory(GraphMemoryInterface):
    """
    Mock Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ².
    ĞĞµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Neo4j, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.
    """
    
    def __init__(self):
        self.episodes: Dict[str, Episode] = {}
        self._initialized = False
    
    async def initialize(self) -> None:
        self._initialized = True
    
    async def add_episode(self, episode: Episode) -> str:
        self.episodes[episode.id] = episode
        return episode.id
    
    async def search(
        self, 
        query: str, 
        limit: int = 10,
        include_deleted: bool = False
    ) -> List[SearchResult]:
        results = []
        query_lower = query.lower()
        
        for ep in self.episodes.values():
            if ep.deleted and not include_deleted:
                continue
            
            if query_lower in ep.content.lower():
                results.append(SearchResult(
                    content=ep.content,
                    relevance_score=1.0,
                    source=ep.source,
                    timestamp=ep.timestamp
                ))
        
        return results[:limit]
    
    async def get_by_id(self, episode_id: str) -> Optional[Episode]:
        return self.episodes.get(episode_id)
    
    async def soft_delete(self, episode_id: str) -> bool:
        if episode_id in self.episodes:
            self.episodes[episode_id].deleted = True
            self.episodes[episode_id].deleted_at = datetime.now()
            return True
        return False
    
    async def hard_delete(self, episode_id: str) -> bool:
        if episode_id in self.episodes:
            del self.episodes[episode_id]
            return True
        return False
    
    async def update_importance(
        self, 
        episode_id: str, 
        new_importance: float
    ) -> bool:
        if episode_id in self.episodes:
            self.episodes[episode_id].importance_score = new_importance
            return True
        return False
    
    async def execute_cypher(
        self, 
        query: str, 
        params: Dict = None
    ) -> List[Dict]:
        # Mock Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Cypher
        return []
    
    async def close(self) -> None:
        self._initialized = False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¤ĞĞ‘Ğ Ğ˜ĞšĞ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_graph_memory(config: Dict, use_mock: bool = False) -> GraphMemoryInterface:
    """
    Ğ¤Ğ°Ğ±Ñ€Ğ¸ĞºĞ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ GraphMemory.
    
    Args:
        config: ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ
        use_mock: True Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ² (Ğ±ĞµĞ· Neo4j)
    
    Returns:
        GraphMemoryInterface Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    """
    if use_mock:
        return MockGraphMemory()
    else:
        return GraphitiAdapter(config)
```

---

## 2ï¸âƒ£ FractalMemory (Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ)

### Ğ¤Ğ°Ğ¹Ğ» `src/core/memory.py`:

```python
"""
FractalMemory â€” Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ñ„Ñ€Ğ°ĞºÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸ĞµĞ¹.

Ğ£Ñ€Ğ¾Ğ²Ğ½Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸:
- L0: Working Memory (Python list, ÑĞµĞºÑƒĞ½Ğ´Ñ‹)
- L1: Short-Term Memory (Python dict, Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹-Ñ‡Ğ°ÑÑ‹)  
- L2: Medium-Term Memory (Graph, Ğ´Ğ½Ğ¸)
- L3: Long-Term Memory (Graph, Ğ¼ĞµÑÑÑ†Ñ‹-Ğ³Ğ¾Ğ´Ñ‹)

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:
    memory = FractalMemory(config)
    await memory.initialize()
    
    # Ğ—Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑŒ
    await memory.remember("ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ»ÑĞ±Ğ¸Ñ‚ Python")
    
    # Ğ’ÑĞ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑŒ
    results = await memory.recall("Ñ‡Ñ‚Ğ¾ Ğ»ÑĞ±Ğ¸Ñ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ")
    
    # ĞšĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ (Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ»Ğ¸ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ)
    await memory.consolidate()
"""

from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
import asyncio
import logging
import uuid
import numpy as np

from .graphiti_adapter import (
    GraphMemoryInterface, 
    GraphitiAdapter,
    Episode,
    SearchResult,
    create_graph_memory
)

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞœĞĞ”Ğ•Ğ›Ğ˜ Ğ”Ğ›Ğ¯ Ğ’ĞĞ£Ğ¢Ğ Ğ•ĞĞĞ•Ğ“Ğ Ğ˜Ğ¡ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞĞ˜Ğ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class MemoryItem:
    """Ğ­Ğ»ĞµĞ¼ĞµĞ½Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ (Ğ´Ğ»Ñ L0/L1)"""
    id: str
    content: str
    embedding: Optional[np.ndarray] = None
    importance: float = 1.0
    access_count: int = 1
    created_at: datetime = field(default_factory=datetime.now)
    last_accessed: datetime = field(default_factory=datetime.now)
    level: int = 0  # 0=L0, 1=L1


@dataclass 
class ConsolidationResult:
    """Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸"""
    promoted: int  # ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ²Ñ‹ÑˆĞµ
    decayed: int   # ĞŸĞ¾Ğ½Ğ¸Ğ¶ĞµĞ½ importance
    deleted: int   # Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¾ (soft delete)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FRACTAL MEMORY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class FractalMemory:
    """
    Ğ¤Ñ€Ğ°ĞºÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸ĞµĞ¹ L0 â†’ L1 â†’ L2 â†’ L3.
    
    ĞŸÑ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹:
    1. Ğ’Ğ°Ğ¶Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑˆĞµ (ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ)
    2. ĞĞµĞ²Ğ°Ğ¶Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ (decay)
    3. Ğ’ÑÑ‘ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ
    """
    
    def __init__(self, config: Dict):
        """
        Args:
            config: {
                "neo4j_uri": "bolt://localhost:7687",
                "neo4j_user": "neo4j",
                "neo4j_password": "password",
                "llm_client": <optional>,
                "embedding_func": <optional callable>,
                
                # ĞĞ¿Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
                "l0_capacity": 10,
                "l1_capacity": 50,
                "decay_rate_l0": 0.1,
                "decay_rate_l1": 0.05,
                "importance_threshold": 0.3,
                "consolidation_interval": 300,  # ÑĞµĞºÑƒĞ½Ğ´Ñ‹
            }
        """
        self.config = config
        
        # Graph adapter (L2/L3)
        self.graph: GraphMemoryInterface = create_graph_memory(config)
        
        # L0: Working Memory (Ğ¾Ñ‡ĞµĞ½ÑŒ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ°Ñ)
        self.l0_cache: List[MemoryItem] = []
        self.l0_capacity = config.get("l0_capacity", 10)
        
        # L1: Short-Term Memory
        self.l1_cache: Dict[str, MemoryItem] = {}
        self.l1_capacity = config.get("l1_capacity", 50)
        
        # Decay rates
        self.decay_rate_l0 = config.get("decay_rate_l0", 0.1)
        self.decay_rate_l1 = config.get("decay_rate_l1", 0.05)
        self.importance_threshold = config.get("importance_threshold", 0.3)
        
        # Embedding function (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
        self.embedding_func = config.get("embedding_func")
        
        # State
        self._initialized = False
        self._consolidation_task = None
    
    async def initialize(self) -> None:
        """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸"""
        if self._initialized:
            return
        
        await self.graph.initialize()
        self._initialized = True
        logger.info("FractalMemory initialized")
    
    async def close(self) -> None:
        """Ğ—Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸"""
        if self._consolidation_task:
            self._consolidation_task.cancel()
        await self.graph.close()
        self._initialized = False
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ĞĞ¡ĞĞĞ’ĞĞ«Ğ• ĞœĞ•Ğ¢ĞĞ”Ğ«
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def remember(
        self, 
        content: str,
        importance: float = 1.0,
        metadata: Dict = None
    ) -> str:
        """
        Ğ—Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ.
        Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ² L0, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ²Ñ‹ÑˆĞµ.
        
        Args:
            content: Ğ§Ñ‚Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑŒ
            importance: ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ (0-1)
            metadata: Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        
        Returns:
            ID ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸
        """
        self._ensure_initialized()
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ embedding ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ
        embedding = None
        if self.embedding_func:
            embedding = await self._get_embedding(content)
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ MemoryItem
        item = MemoryItem(
            id=self._generate_id(),
            content=content,
            embedding=embedding,
            importance=importance,
            created_at=datetime.now(),
            last_accessed=datetime.now(),
            level=0
        )
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ² L0
        self.l0_cache.append(item)
        logger.debug(f"Added to L0: {item.id[:8]}...")
        
        # Ğ•ÑĞ»Ğ¸ L0 Ğ¿ĞµÑ€ĞµĞ¿Ğ¾Ğ»Ğ½ĞµĞ½ â€” Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸
        if len(self.l0_cache) > self.l0_capacity:
            await self._consolidate_l0_to_l1()
        
        return item.id
    
    async def recall(
        self, 
        query: str,
        limit: int = 5,
        levels: List[int] = None
    ) -> List[SearchResult]:
        """
        Ğ’ÑĞ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ.
        Ğ˜Ñ‰ĞµÑ‚ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.
        
        Args:
            query: Ğ§Ñ‚Ğ¾ Ğ¸ÑĞºĞ°Ñ‚ÑŒ
            limit: ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
            levels: ĞšĞ°ĞºĞ¸Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ¸ÑĞºĞ°Ñ‚ÑŒ [0,1,2,3] Ğ¸Ğ»Ğ¸ None Ğ´Ğ»Ñ Ğ²ÑĞµÑ…
        
        Returns:
            Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¾Ñ‚ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸
        """
        self._ensure_initialized()
        
        if levels is None:
            levels = [0, 1, 2, 3]
        
        all_results = []
        
        # L0: Ğ¿Ğ¾Ğ¸ÑĞº Ğ² working memory
        if 0 in levels:
            l0_results = self._search_l0(query)
            all_results.extend(l0_results)
        
        # L1: Ğ¿Ğ¾Ğ¸ÑĞº Ğ² short-term
        if 1 in levels:
            l1_results = self._search_l1(query)
            all_results.extend(l1_results)
        
        # L2/L3: Ğ¿Ğ¾Ğ¸ÑĞº Ğ² Ğ³Ñ€Ğ°Ñ„Ğµ
        if 2 in levels or 3 in levels:
            graph_results = await self.graph.search(query, limit=limit * 2)
            all_results.extend(graph_results)
        
        # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾ relevance
        all_results.sort(key=lambda x: x.relevance_score, reverse=True)
        
        # ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ access_count Ğ´Ğ»Ñ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ…
        await self._update_access_counts(all_results[:limit])
        
        return all_results[:limit]
    
    async def consolidate(self) -> ConsolidationResult:
        """
        ĞšĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: L0 â†’ L1 â†’ L2 â†’ L3.
        Ğ’Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ»Ğ¸ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ.
        
        Returns:
            Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸
        """
        self._ensure_initialized()
        
        result = ConsolidationResult(promoted=0, decayed=0, deleted=0)
        
        # L0 â†’ L1
        r1 = await self._consolidate_l0_to_l1()
        result.promoted += r1.promoted
        result.decayed += r1.decayed
        result.deleted += r1.deleted
        
        # L1 â†’ L2
        r2 = await self._consolidate_l1_to_l2()
        result.promoted += r2.promoted
        result.decayed += r2.decayed
        result.deleted += r2.deleted
        
        # Apply decay to all levels
        await self._apply_decay()
        
        logger.info(
            f"Consolidation complete: "
            f"promoted={result.promoted}, "
            f"decayed={result.decayed}, "
            f"deleted={result.deleted}"
        )
        
        return result
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ĞšĞĞĞ¡ĞĞ›Ğ˜Ğ”ĞĞ¦Ğ˜Ğ¯
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def _consolidate_l0_to_l1(self) -> ConsolidationResult:
        """ĞšĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ L0 â†’ L1"""
        result = ConsolidationResult(promoted=0, decayed=0, deleted=0)
        now = datetime.now()
        
        items_to_remove = []
        
        for item in self.l0_cache:
            age_minutes = (now - item.created_at).total_seconds() / 60
            
            # Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ importance Ñ decay
            new_importance = self._calculate_importance(item, age_minutes)
            item.importance = new_importance
            
            if new_importance > 0.7 or item.access_count > 2:
                # ĞŸĞ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ğ´Ğ¾ L1
                item.level = 1
                self.l1_cache[item.id] = item
                items_to_remove.append(item)
                result.promoted += 1
                logger.debug(f"Promoted to L1: {item.id[:8]}...")
                
            elif new_importance < self.importance_threshold:
                # Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ (Ğ·Ğ°Ğ±Ñ‹Ñ‚ÑŒ)
                items_to_remove.append(item)
                result.deleted += 1
                logger.debug(f"Forgotten from L0: {item.id[:8]}...")
                
            else:
                result.decayed += 1
        
        # Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ
        for item in items_to_remove:
            if item in self.l0_cache:
                self.l0_cache.remove(item)
        
        return result
    
    async def _consolidate_l1_to_l2(self) -> ConsolidationResult:
        """ĞšĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ L1 â†’ L2 (Ğ³Ñ€Ğ°Ñ„)"""
        result = ConsolidationResult(promoted=0, decayed=0, deleted=0)
        now = datetime.now()
        
        items_to_remove = []
        
        for item_id, item in list(self.l1_cache.items()):
            age_hours = (now - item.created_at).total_seconds() / 3600
            
            # Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ importance
            new_importance = self._calculate_importance(item, age_hours * 60)
            item.importance = new_importance
            
            # ĞšÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ»Ñ L2:
            # - Ğ’Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ > 0.7
            # - Ğ˜Ğ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ > 5 Ñ€Ğ°Ğ·
            # - Ğ˜Ğ»Ğ¸ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚ > 1 Ñ‡Ğ°Ñ Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ > 0.5
            should_promote = (
                new_importance > 0.7 or 
                item.access_count > 5 or
                (age_hours > 1 and new_importance > 0.5)
            )
            
            if should_promote:
                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ² Ğ³Ñ€Ğ°Ñ„
                episode = Episode(
                    id=item.id,
                    content=item.content,
                    timestamp=item.created_at,
                    source="l1_consolidation",
                    importance_score=new_importance,
                    level=2,
                    metadata={"access_count": item.access_count}
                )
                await self.graph.add_episode(episode)
                
                items_to_remove.append(item_id)
                result.promoted += 1
                logger.debug(f"Promoted to L2: {item_id[:8]}...")
                
            elif new_importance < self.importance_threshold and age_hours > 2:
                # Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ (Ğ·Ğ°Ğ±Ñ‹Ñ‚ÑŒ)
                items_to_remove.append(item_id)
                result.deleted += 1
                
            else:
                result.decayed += 1
        
        # Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ
        for item_id in items_to_remove:
            self.l1_cache.pop(item_id, None)
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ capacity
        if len(self.l1_cache) > self.l1_capacity:
            # Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ğ½Ğ°Ğ¸Ğ¼ĞµĞ½ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ
            sorted_items = sorted(
                self.l1_cache.items(),
                key=lambda x: x[1].importance
            )
            to_remove = len(self.l1_cache) - self.l1_capacity
            for item_id, _ in sorted_items[:to_remove]:
                self.l1_cache.pop(item_id, None)
                result.deleted += 1
        
        return result
    
    async def _apply_decay(self) -> None:
        """ĞŸÑ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ decay ĞºĞ¾ Ğ²ÑĞµĞ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼"""
        now = datetime.now()
        
        # L0 decay
        for item in self.l0_cache:
            age_minutes = (now - item.last_accessed).total_seconds() / 60
            item.importance *= np.exp(-self.decay_rate_l0 * age_minutes / 60)
        
        # L1 decay
        for item in self.l1_cache.values():
            age_hours = (now - item.last_accessed).total_seconds() / 3600
            item.importance *= np.exp(-self.decay_rate_l1 * age_hours)
        
        # L2/L3 decay Ğ² Ğ³Ñ€Ğ°Ñ„Ğµ (Ñ‡ĞµÑ€ĞµĞ· Cypher)
        await self.graph.execute_cypher(
            """
            MATCH (ep:Episode)
            WHERE ep.deleted = false
              AND ep.last_accessed < datetime() - duration({hours: 24})
            SET ep.importance_score = ep.importance_score * 0.95
            """,
            {}
        )
    
    def _calculate_importance(
        self, 
        item: MemoryItem, 
        age_minutes: float
    ) -> float:
        """
        Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ importance Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼:
        - Temporal decay
        - Access count (reinforcement)
        """
        # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ decay
        decay_rate = self.decay_rate_l0 if item.level == 0 else self.decay_rate_l1
        temporal_decay = np.exp(-decay_rate * age_minutes / 60)
        
        # Reinforcement: Ñ‡Ğ°ÑÑ‚Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğ·Ğ°Ğ¼ĞµĞ´Ğ»ÑĞµÑ‚ decay
        reinforcement = 1.0 + np.log1p(item.access_count) * 0.1
        
        # Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ importance
        new_importance = item.importance * temporal_decay * reinforcement
        
        return max(0.0, min(1.0, new_importance))
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ĞŸĞĞ˜Ğ¡Ğš
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _search_l0(self, query: str) -> List[SearchResult]:
        """ĞŸĞ¾Ğ¸ÑĞº Ğ² L0 (Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ keyword matching)"""
        results = []
        query_lower = query.lower()
        
        for item in self.l0_cache:
            if query_lower in item.content.lower():
                results.append(SearchResult(
                    content=item.content,
                    relevance_score=item.importance,
                    source="l0",
                    timestamp=item.created_at,
                    metadata={"level": 0}
                ))
        
        return results
    
    def _search_l1(self, query: str) -> List[SearchResult]:
        """ĞŸĞ¾Ğ¸ÑĞº Ğ² L1"""
        results = []
        query_lower = query.lower()
        
        for item in self.l1_cache.values():
            if query_lower in item.content.lower():
                results.append(SearchResult(
                    content=item.content,
                    relevance_score=item.importance,
                    source="l1",
                    timestamp=item.created_at,
                    metadata={"level": 1}
                ))
        
        return results
    
    async def _update_access_counts(self, results: List[SearchResult]) -> None:
        """ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑÑ‡Ñ‘Ñ‚Ñ‡Ğ¸ĞºĞ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²"""
        for result in results:
            level = result.metadata.get("level", 2)
            
            if level == 0:
                # L0
                for item in self.l0_cache:
                    if item.content == result.content:
                        item.access_count += 1
                        item.last_accessed = datetime.now()
                        break
                        
            elif level == 1:
                # L1
                for item in self.l1_cache.values():
                    if item.content == result.content:
                        item.access_count += 1
                        item.last_accessed = datetime.now()
                        break
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ’Ğ¡ĞŸĞĞœĞĞ“ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ• ĞœĞ•Ğ¢ĞĞ”Ğ«
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def _get_embedding(self, text: str) -> np.ndarray:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ embedding Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ°"""
        if self.embedding_func:
            return await self.embedding_func(text)
        return None
    
    def _generate_id(self) -> str:
        """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ID"""
        return str(uuid.uuid4())
    
    def _ensure_initialized(self):
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ"""
        if not self._initialized:
            raise RuntimeError(
                "FractalMemory not initialized. "
                "Call await memory.initialize() first."
            )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_stats(self) -> Dict:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸"""
        return {
            "l0_size": len(self.l0_cache),
            "l0_capacity": self.l0_capacity,
            "l1_size": len(self.l1_cache),
            "l1_capacity": self.l1_capacity,
            "l0_avg_importance": np.mean([i.importance for i in self.l0_cache]) if self.l0_cache else 0,
            "l1_avg_importance": np.mean([i.importance for i in self.l1_cache.values()]) if self.l1_cache else 0,
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GARBAGE COLLECTION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def garbage_collect(self, soft_delete_age_days: int = 7) -> Dict:
        """
        Garbage Collection:
        1. Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ soft deleted ÑÑ‚Ğ°Ñ€ÑˆĞµ N Ğ´Ğ½ĞµĞ¹
        2. Soft delete Ğ½Ğ¸Ğ·ĞºĞ¾-Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸
        
        Args:
            soft_delete_age_days: Ğ§ĞµÑ€ĞµĞ· ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ½ĞµĞ¹ Ğ¿Ğ¾ÑĞ»Ğµ soft delete ÑƒĞ´Ğ°Ğ»ÑÑ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸
        
        Returns:
            Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° GC
        """
        self._ensure_initialized()
        
        stats = {
            "hard_deleted": 0,
            "soft_deleted": 0
        }
        
        # 1. Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ soft deleted + ÑÑ‚Ğ°Ñ€Ñ‹Ğµ)
        hard_delete_result = await self.graph.execute_cypher(
            """
            MATCH (n)
            WHERE n.deleted = true
              AND n.deleted_at < datetime() - duration({days: $days})
            WITH n, labels(n) as labels
            DETACH DELETE n
            RETURN count(n) as deleted_count
            """,
            {"days": soft_delete_age_days}
        )
        stats["hard_deleted"] = hard_delete_result[0]["deleted_count"] if hard_delete_result else 0
        
        # 2. Soft delete Ğ½Ğ¸Ğ·ĞºĞ¾-Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ…
        soft_delete_result = await self.graph.execute_cypher(
            """
            MATCH (ep:Episode)
            WHERE ep.deleted = false
              AND ep.importance_score < $threshold
              AND ep.access_count = 0
              AND ep.timestamp < datetime() - duration({days: 30})
            SET ep.deleted = true,
                ep.deleted_at = datetime()
            RETURN count(ep) as deleted_count
            """,
            {"threshold": self.importance_threshold}
        )
        stats["soft_deleted"] = soft_delete_result[0]["deleted_count"] if soft_delete_result else 0
        
        logger.info(f"GC complete: {stats}")
        return stats


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¤ĞĞ‘Ğ Ğ˜ĞšĞ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_fractal_memory(config: Dict) -> FractalMemory:
    """
    Ğ¤Ğ°Ğ±Ñ€Ğ¸ĞºĞ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ FractalMemory.
    
    Args:
        config: ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ (ÑĞ¼. FractalMemory.__init__)
    
    Returns:
        ĞĞ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ FractalMemory
    """
    return FractalMemory(config)
```

---

## 3ï¸âƒ£ Ğ¢ĞµÑÑ‚Ñ‹

### Ğ¤Ğ°Ğ¹Ğ» `tests/test_memory.py`:

```python
"""
Unit Ñ‚ĞµÑÑ‚Ñ‹ Ğ´Ğ»Ñ FractalMemory.

Ğ—Ğ°Ğ¿ÑƒÑĞº:
    pytest tests/test_memory.py -v
"""

import pytest
import asyncio
from datetime import datetime, timedelta
from src.core.memory import FractalMemory, create_fractal_memory
from src.core.graphiti_adapter import MockGraphMemory


@pytest.fixture
def config():
    """ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ² (Ñ mock)"""
    return {
        "neo4j_uri": "bolt://localhost:7687",
        "neo4j_user": "neo4j",
        "neo4j_password": "test",
        "l0_capacity": 5,
        "l1_capacity": 10,
        "importance_threshold": 0.3,
    }


@pytest.fixture
async def memory(config):
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ FractalMemory Ñ mock Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ¼"""
    mem = FractalMemory(config)
    mem.graph = MockGraphMemory()  # ĞŸĞ¾Ğ´Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ½Ğ° mock
    await mem.initialize()
    yield mem
    await mem.close()


class TestFractalMemory:
    """Ğ¢ĞµÑÑ‚Ñ‹ FractalMemory"""
    
    @pytest.mark.asyncio
    async def test_remember_adds_to_l0(self, memory):
        """remember() Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ² L0"""
        item_id = await memory.remember("test content")
        
        assert len(memory.l0_cache) == 1
        assert memory.l0_cache[0].id == item_id
        assert memory.l0_cache[0].content == "test content"
    
    @pytest.mark.asyncio
    async def test_recall_finds_in_l0(self, memory):
        """recall() Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² L0"""
        await memory.remember("Python is great")
        
        results = await memory.recall("Python")
        
        assert len(results) == 1
        assert "Python" in results[0].content
    
    @pytest.mark.asyncio
    async def test_consolidation_promotes_to_l1(self, memory):
        """ĞšĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ² L1"""
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ
        await memory.remember("important info", importance=0.9)
        
        # ĞšĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
        result = await memory.consolidate()
        
        assert result.promoted >= 1
        assert len(memory.l1_cache) >= 1
    
    @pytest.mark.asyncio
    async def test_l0_overflow_triggers_consolidation(self, memory):
        """ĞŸĞµÑ€ĞµĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ L0 Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€Ğ¸Ñ‚ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ"""
        # Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ L0 Ğ²Ñ‹ÑˆĞµ capacity
        for i in range(memory.l0_capacity + 2):
            await memory.remember(f"content {i}", importance=0.8)
        
        # L0 Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ğ» ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ
        assert len(memory.l0_cache) <= memory.l0_capacity
    
    @pytest.mark.asyncio
    async def test_soft_delete_works(self, memory):
        """Soft delete Ğ¿Ğ¾Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ"""
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ² Ğ³Ñ€Ğ°Ñ„
        await memory.remember("to delete", importance=0.9)
        await memory.consolidate()
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ñ‡Ñ‚Ğ¾ ĞµÑÑ‚ÑŒ Ğ² Ğ³Ñ€Ğ°Ñ„Ğµ
        # (Ñ‡ĞµÑ€ĞµĞ· mock ÑÑ‚Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾, Ğ½Ğ¾ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚)
        
        # GC Ñ soft delete
        stats = await memory.garbage_collect()
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ» Ğ±ĞµĞ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº
        assert "soft_deleted" in stats
        assert "hard_deleted" in stats
    
    @pytest.mark.asyncio
    async def test_stats_returns_correct_data(self, memory):
        """get_stats() Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ"""
        await memory.remember("test 1")
        await memory.remember("test 2")
        
        stats = memory.get_stats()
        
        assert stats["l0_size"] == 2
        assert stats["l0_capacity"] == memory.l0_capacity
        assert stats["l1_size"] == 0


class TestConsolidation:
    """Ğ¢ĞµÑÑ‚Ñ‹ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸"""
    
    @pytest.mark.asyncio
    async def test_low_importance_forgotten(self, memory):
        """ĞĞ¸Ğ·ĞºĞ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ â†’ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ"""
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ
        await memory.remember("unimportant", importance=0.1)
        
        # Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ¾ÑÑ‚Ğ°Ñ€Ğ¸Ñ‚ÑŒ
        memory.l0_cache[0].created_at = datetime.now() - timedelta(hours=1)
        
        # ĞšĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
        result = await memory.consolidate()
        
        # Ğ”Ğ¾Ğ»Ğ¶Ğ½Ğ¾ Ğ±Ñ‹Ñ‚ÑŒ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¾ Ğ¸Ğ»Ğ¸ decayed
        assert result.deleted >= 1 or result.decayed >= 1
    
    @pytest.mark.asyncio
    async def test_high_access_promotes(self, memory):
        """Ğ§Ğ°ÑÑ‚Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ"""
        await memory.remember("popular content", importance=0.5)
        
        # Ğ˜Ğ¼Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡Ğ°ÑÑ‚Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿
        memory.l0_cache[0].access_count = 5
        
        # ĞšĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
        result = await memory.consolidate()
        
        assert result.promoted >= 1
```

---

## âœ… ĞšÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Phase 2

- [ ] `GraphitiAdapter` ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚
- [ ] `FractalMemory` ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚
- [ ] `remember()` Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ² L0
- [ ] `recall()` Ğ¸Ñ‰ĞµÑ‚ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼
- [ ] ĞšĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ L0â†’L1â†’L2 Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚
- [ ] Soft delete Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚
- [ ] Unit Ñ‚ĞµÑÑ‚Ñ‹ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ÑÑ‚: `pytest tests/test_memory.py -v`

---

## ğŸ“š Ğ¡Ğ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ ÑˆĞ°Ğ³

ĞŸĞµÑ€ĞµĞ¹Ğ´Ğ¸ Ğº: **[04_PHASE3_LEARNING.md](04_PHASE3_LEARNING.md)** â€” ReasoningBank Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
