"""
FractalAgent ‚Äî –≥–ª–∞–≤–Ω—ã–π —Ñ–∞—Å–∞–¥ –¥–ª—è AI-–∞–≥–µ–Ω—Ç–∞ —Å —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ–π –ø–∞–º—è—Ç—å—é.

–û–±—ä–µ–¥–∏–Ω—è–µ—Ç:
- FractalMemory –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è
- HybridRetriever –¥–ª—è —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
- ReasoningBank –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è
- LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
"""

import asyncio
import logging
import os
import time
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

from src.infrastructure.rate_limiter import RateLimiter, rate_limit
from src.infrastructure.metrics import (
    memory_size,
    retrieval_latency,
    tokens_per_query,
    tokens_used,
)

logger = logging.getLogger(__name__)


class AgentState(Enum):
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞."""
    IDLE = "idle"
    THINKING = "thinking"
    RESPONDING = "responding"
    LEARNING = "learning"
    ERROR = "error"


@dataclass
class ChatMessage:
    """–°–æ–æ–±—â–µ–Ω–∏–µ –≤ —á–∞—Ç–µ."""
    role: str  # "user" –∏–ª–∏ "assistant"
    content: str
    timestamp: datetime = field(default_factory=datetime.now)
    metadata: Dict = field(default_factory=dict)


@dataclass
class AgentResponse:
    """–û—Ç–≤–µ—Ç –∞–≥–µ–Ω—Ç–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏."""
    content: str
    context_used: List[Dict] = field(default_factory=list)
    strategies_used: List[str] = field(default_factory=list)
    memory_stats: Dict = field(default_factory=dict)
    processing_time_ms: float = 0.0


class FractalAgent:
    """
    AI-–∞–≥–µ–Ω—Ç —Å —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ–π –ø–∞–º—è—Ç—å—é.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –ø–∞–º—è—Ç—å (L0-L3) –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞,
    –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏,
    –∏ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—ã—Ç–∞.
    
    Example:
        ```python
        agent = FractalAgent(config)
        await agent.initialize()
        
        response = await agent.chat("–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç –°–µ—Ä–≥–µ–π")
        print(response.content)
        
        await agent.close()
        ```
    """
    
    DEFAULT_CONFIG = {
        # Memory
        "neo4j_uri": "bolt://localhost:7687",
        "neo4j_user": "neo4j",
        # neo4j_password –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–¥–∞–Ω —è–≤–Ω–æ (–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å)
        "redis_url": "redis://localhost:6379",
        
        # LLM
        "openai_api_key": None,  # –ò–∑ env
        "model": "gpt-5-mini",
        "max_tokens": 5000,  # –£–≤–µ–ª–∏—á–µ–Ω–æ: gpt-5-nano –∏—Å–ø–æ–ª—å–∑—É–µ—Ç reasoning tokens, –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –º–µ—Å—Ç–∞
        # "temperature": 0.7,  # –£–±—Ä–∞–Ω–æ - gpt-5-nano-2025-08-07 –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–µ—Ñ–æ–ª—Ç 1
        
        # Retrieval
        "retrieval_limit": 5,
        "retrieval_weights": {
            "vector": 0.5,
            "keyword": 0.3,
            "graph": 0.2,
        },
        "llm_requests_per_minute": 60,
        
        # Agent
        "system_prompt": """
You are Mark, an intelligent AI partner.
Your goal is to be a thoughtful listener and a strategic partner.

CORE BEHAVIORS:

1. **Mirror the User's Intent:**
   - If the user is sharing, telling a story, or stating a fact -> **LISTEN**, briefly acknowledge, and react exactly as a real person would.
   - If the user asks a question -> Answer precisely and to the point.
   - If the user explicitly asks for help -> Be proactive and offer solutions.

2. **Natural Memory usage:**
   - NEVER say "I remember you said...".
   - Simply weave the knowledge from context into your response naturally, just like a human friend who knows you would.

3. **Communication Style:**
   - Concise, natural, no fluff.
   - NO bullet points for casual chit-chat.
   - NO checklists unless explicitly requested.
   - Avoid generic phrases like "How can I help you today?" in the middle of a conversation.
   - Example: If the user says "I go to the BMX track on Wednesdays", reply simply: "Cool, noted."

Your personality is calm, professional, and attentive.
""",
        
        "save_all_messages": True,
        "learn_from_interactions": True,
    }
    
    def __init__(
        self,
        config: Optional[Dict] = None,
        memory: Optional["FractalMemory"] = None,
        retriever: Optional["HybridRetriever"] = None,
        reasoning: Optional["ReasoningBank"] = None,
        **kwargs
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–≥–µ–Ω—Ç–∞.
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è defaults)
            memory: –ì–æ—Ç–æ–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä FractalMemory (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            retriever: –ì–æ—Ç–æ–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä HybridRetriever (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            reasoning: –ì–æ—Ç–æ–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä ReasoningBank (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        
        Note:
            –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –≥–æ—Ç–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (memory, retriever, reasoning),
            –æ–Ω–∏ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –≤–º–µ—Å—Ç–æ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤—ã—Ö. –≠—Ç–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è —Ç–µ—Å—Ç–æ–≤
            –∏ –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è.
        """
        # Merge config with kwargs for backward compatibility
        merged_config = {**self.DEFAULT_CONFIG, **(config or {}), **kwargs}
        self.config = merged_config
        
        # User identity
        self.user_id = self.config.get("user_id", "default")
        self.user_name = self.config.get("user_name", "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å")
        self.agent_name = self.config.get("agent_name", "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç")
        
        # User context (–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ)
        self.user_context: Dict[str, Any] = {}
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –≤ initialize() –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ)
        self.memory = memory
        self.retriever = retriever
        self.reasoning = reasoning
        self.llm_client = None
        
        # Track component ownership for proper cleanup
        self._owns_memory = memory is None
        self._owns_retriever = retriever is None
        self._owns_reasoning = reasoning is None
        
        # Rate limiter
        self.llm_rate_limiter: Optional[RateLimiter] = None
        rpm = self.config.get("llm_requests_per_minute")
        if rpm:
            try:
                rpm_value = int(rpm)
                if rpm_value > 0:
                    self.llm_rate_limiter = RateLimiter(rate=rpm_value, per_seconds=60)
            except Exception:
                logger.warning("Invalid llm_requests_per_minute=%s, rate limiting disabled", rpm)
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ
        self.state = AgentState.IDLE
        self.conversation_history: List[ChatMessage] = []
        self._initialized = False
        
        # Log if using provided components
        if memory is not None:
            logger.info("FractalAgent initialized with provided FractalMemory instance")
        if retriever is not None:
            logger.info("FractalAgent initialized with provided HybridRetriever instance")
        if reasoning is not None:
            logger.info("FractalAgent initialized with provided ReasoningBank instance")
    
    async def initialize(self) -> None:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã.
        
        –î–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤—ã–∑–≤–∞–Ω –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≥–µ–Ω—Ç–∞.
        """
        if self._initialized:
            return
        
        logger.info("Initializing FractalAgent...")
        
        try:
            # –ü–µ—Ä–µ–¥–∞—Ç—å user_id –≤ –ø–∞–º—è—Ç—å
            self.config["user_id"] = self.user_id
            
            # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–∞–º—è—Ç—å (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é)
            if self.memory is None:
                from src.core.memory import FractalMemory
                self.memory = FractalMemory(self.config)
                await self.memory.initialize()
                logger.info("FractalMemory initialized (created new)")
            else:
                logger.info("Using provided FractalMemory instance")
                # Ensure memory is initialized
                if not getattr(self.memory, '_initialized', False):
                    await self.memory.initialize()
                    logger.info("Provided FractalMemory initialized")
            
            # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å retriever (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π)
            if self.retriever is None:
                from src.core.retrieval import HybridRetriever
                self.retriever = HybridRetriever(
                    self.memory.graphiti,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º GraphitiStore –∏–∑ memory
                    user_id=self.user_id,
                    weights=self.config.get("retrieval_weights"),
                )
                logger.info("HybridRetriever initialized (created new)")
            else:
                logger.info("Using provided HybridRetriever instance")
            
            # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å ReasoningBank (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π)
            if self.reasoning is None:
                from src.core.reasoning import ReasoningBank
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º GraphitiStore –∏–∑ memory –≤–º–µ—Å—Ç–æ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ –¥—Ä–∞–π–≤–µ—Ä–∞
                self.reasoning = ReasoningBank(self.memory.graphiti, self.user_id)
                await self.reasoning.initialize()
                logger.info("ReasoningBank initialized (created new)")
            else:
                logger.info("Using provided ReasoningBank instance")
                # Ensure reasoning is initialized
                if not getattr(self.reasoning, '_initialized', False):
                    await self.reasoning.initialize()
                    logger.info("Provided ReasoningBank initialized")
            
            # 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LLM –∫–ª–∏–µ–Ω—Ç
            await self._init_llm_client()
            logger.info("LLM client initialized")
            
            # 5. –ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            await self._load_user_context()
            
            self._initialized = True
            self.state = AgentState.IDLE
            logger.info(f"FractalAgent fully initialized for user {self.user_id}")
            
        except Exception as e:
            self.state = AgentState.ERROR
            component = self._identify_failed_component(e)
            logger.error(
                f"Failed to initialize agent at component: {component}",
                extra={
                    "component": component,
                    "error": str(e),
                    "user_id": self.user_id,
                },
                exc_info=True
            )
            raise RuntimeError(
                f"FractalAgent initialization failed at component: {component}. "
                f"Error: {str(e)}"
            ) from e
    
    def _identify_failed_component(self, error: Exception) -> str:
        """Identify which component failed based on error."""
        error_str = str(error).lower()
        
        if "neo4j" in error_str or "graphiti" in error_str:
            return "GraphitiStore (Neo4j connection)"
        elif "redis" in error_str:
            return "RedisMemoryStore (Redis connection)"
        elif "openai" in error_str or "api key" in error_str:
            return "LLM Client (OpenAI)"
        elif "memory" in error_str:
            return "FractalMemory"
        elif "retriever" in error_str:
            return "HybridRetriever"
        elif "reasoning" in error_str:
            return "ReasoningBank"
        else:
            return "Unknown component"
    
    async def _init_llm_client(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LLM –∫–ª–∏–µ–Ω—Ç."""
        try:
            from openai import AsyncOpenAI
            import httpx
            
            api_key = self.config.get("openai_api_key") or os.getenv("OPENAI_API_KEY")
            if not api_key:
                logger.warning("No OpenAI API key found, LLM features disabled")
                self.llm_client = None
                return
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–∞–π–º–∞—É—Ç–æ–≤ –¥–ª—è httpx –∫–ª–∏–µ–Ω—Ç–∞
            # connect: –≤—Ä–µ–º—è –Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–∫—É —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            # read: –≤—Ä–µ–º—è –Ω–∞ —á—Ç–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è LLM)
            # write: –≤—Ä–µ–º—è –Ω–∞ –æ—Ç–ø—Ä–∞–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞
            # pool: –≤—Ä–µ–º—è –Ω–∞ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –∏–∑ –ø—É–ª–∞
            timeout = httpx.Timeout(
                connect=10.0,  # 10 —Å–µ–∫—É–Ω–¥ –Ω–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
                read=50.0,     # 50 —Å–µ–∫—É–Ω–¥ –Ω–∞ —á—Ç–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ (—É–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è gpt-5-nano —Å reasoning)
                write=10.0,    # 10 —Å–µ–∫—É–Ω–¥ –Ω–∞ –æ—Ç–ø—Ä–∞–≤–∫—É
                pool=5.0       # 5 —Å–µ–∫—É–Ω–¥ –Ω–∞ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            )
            
            self.llm_client = AsyncOpenAI(
                api_key=api_key,
                timeout=timeout,
                max_retries=2  # –ú–∞–∫—Å–∏–º—É–º 2 –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
            )
            logger.info("LLM client initialized with timeouts: connect=10s, read=30s")
        except ImportError:
            logger.warning("openai package not installed, LLM features disabled")
            self.llm_client = None
    
    async def _load_user_context(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ."""
        try:
            # –°–Ω–∞—á–∞–ª–∞ –∏–∑ Redis (L0/L1)
            redis_facts = []
            if self.memory and self.memory.redis_store:
                redis_facts = await self.memory.redis_store.search_l0_l1(
                    self.user_name, limit=5
                )
            
            # –ó–∞—Ç–µ–º –∏–∑ Neo4j (L2)
            neo4j_facts = []
            if self.memory and self.memory.graphiti:
                try:
                    user_tag = f"[user:{self.user_id}]"
                    neo4j_results = await self.memory.graphiti.execute_cypher(
                        """
                        MATCH (ep:Episodic)
                        WHERE ep.content CONTAINS $user_tag
                        RETURN ep.content as content
                        ORDER BY ep.created_at DESC
                        LIMIT 10
                        """,
                        {"user_tag": user_tag}
                    )
                    neo4j_facts = [r.get("content", "") for r in (neo4j_results or [])]
                except Exception as e:
                    logger.warning(f"Failed to load Neo4j facts: {e}")
            
            self.user_context = {
                "user_id": self.user_id,
                "user_name": self.user_name,
                "agent_name": self.agent_name,
                "redis_facts": [f.get("content", "") for f in redis_facts],
                "neo4j_facts": neo4j_facts,
            }
            
            total_facts = len(redis_facts) + len(neo4j_facts)
            logger.info(f"Loaded user context: {total_facts} facts")
            
        except Exception as e:
            logger.warning(f"Failed to load user context: {e}")
            self.user_context = {
                "user_id": self.user_id,
                "user_name": self.user_name,
                "agent_name": self.agent_name,
            }
    
    def _build_system_prompt(self) -> str:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."""
        prompt = f"""–¢—ã ‚Äî {self.agent_name}, AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é.
–¢—ã –ø–æ–º–Ω–∏—à—å –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–∞–∑–≥–æ–≤–æ—Ä—ã –∏ –º–æ–∂–µ—à—å —É—á–∏—Ç—å—Å—è –Ω–∞ –æ–ø—ã—Ç–µ.

–¢–í–û–Ø –ò–î–ï–ù–¢–ò–ß–ù–û–°–¢–¨:
- –¢–≤–æ—ë –∏–º—è: {self.agent_name}
- –ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {self.user_name}
- –¢—ã —Å–æ–∑–¥–∞–Ω {self.user_name} –∫–∞–∫ –ø–æ–º–æ—â–Ω–∏–∫ –∏ –ø–∞—Ä—Ç–Ω—ë—Ä
"""
        
        # –î–æ–±–∞–≤–∏—Ç—å –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã
        all_facts = (
            self.user_context.get("redis_facts", []) +
            self.user_context.get("neo4j_facts", [])
        )
        
        if all_facts:
            prompt += "\n–ò–ó–í–ï–°–¢–ù–´–ï –§–ê–ö–¢–´ –û –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï:\n"
            for fact in all_facts[:5]:
                prompt += f"- {fact[:200]}\n"
        
        prompt += """
–ò–ù–°–¢–†–£–ö–¶–ò–ò:
- –û–±—Ä–∞—â–∞–π—Å—è –∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –ø–æ –∏–º–µ–Ω–∏
- –ü–æ–º–Ω–∏ —Å–≤–æ—é –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å
- –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø–∞–º—è—Ç–∏ –≤ –æ—Ç–≤–µ—Ç–∞—Ö
- –ù–ï —Å–ø—Ä–∞—à–∏–≤–∞–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–ø–æ–º–Ω–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é - —Ç—ã –∑–∞–ø–æ–º–∏–Ω–∞–µ—à—å –≤—Å—ë –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
- –ù–ï –ø—Ä–µ–¥–ª–∞–≥–∞–π "–∑–∞–ø–æ–º–Ω–∏—Ç—å" –∏–ª–∏ "—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å" —Ñ–∞–∫—Ç—ã - —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤ —Ñ–æ–Ω–µ
- –ü—Ä–æ—Å—Ç–æ –æ–±—â–∞–π—Å—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ, –∫–∞–∫ —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø–∞–º—è—Ç–∏ –∫–æ–≥–¥–∞ –æ–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞
"""
        
        return prompt
    
    async def chat(
        self,
        message: str,
        metadata: Optional[Dict] = None,
    ) -> AgentResponse:
        """
        –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        
        Args:
            message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        
        Returns:
            AgentResponse —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if not self._initialized:
            await self.initialize()
        
        start_time = datetime.now()
        self.state = AgentState.THINKING
        
        try:
            # 1. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_message = ChatMessage(
                role="user",
                content=message,
                metadata=metadata or {},
            )
            self.conversation_history.append(user_message)
            
            # 2. –ù–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context = await self._retrieve_context(message)
            
            # 3. –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            strategies = await self._get_strategies(message)
            
            # 4. –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç
            self.state = AgentState.RESPONDING
            response_text = await self._generate_response(
                message, context, strategies
            )
            
            # 5. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ø–∞–º—è—Ç—å (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç)
            # Graphiti add_episode –º–æ–∂–µ—Ç –∑–∞–Ω–∏–º–∞—Ç—å 10-16 —Å–µ–∫—É–Ω–¥, –ø–æ—ç—Ç–æ–º—É –∑–∞–ø—É—Å–∫–∞–µ–º –≤ —Ñ–æ–Ω–µ
            if self.config.get("save_all_messages", True):
                # –°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á—É, –Ω–æ –Ω–µ –∂–¥—ë–º –µ—ë –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                asyncio.create_task(self._save_to_memory(message, response_text, metadata))
                # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ (–±—ã—Å—Ç—Ä–æ)
                self._update_memory_metrics()
            
            # 6. –ó–∞–ø–∏—Å–∞—Ç—å –æ–ø—ã—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (—Ç–æ–∂–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)
            if self.config.get("learn_from_interactions", True):
                self.state = AgentState.LEARNING
                # –°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á—É, –Ω–æ –Ω–µ –∂–¥—ë–º –µ—ë –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                asyncio.create_task(self._log_experience(message, response_text, context, next_user_message=None))
            
            # 7. –î–æ–±–∞–≤–∏—Ç—å –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é
            assistant_message = ChatMessage(
                role="assistant",
                content=response_text,
                metadata={"context_count": len(context)},
            )
            self.conversation_history.append(assistant_message)
            
            # –°–æ–±—Ä–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            memory_stats = await self.get_stats()
            
            self.state = AgentState.IDLE
            
            return AgentResponse(
                content=response_text,
                context_used=[
                    {"content": c.content[:100], "score": c.score, "source": c.source}
                    for c in context[:5]
                ],
                strategies_used=[s.description[:50] for s in strategies[:3]] if strategies else [],
                memory_stats=memory_stats,
                processing_time_ms=processing_time,
            )
            
        except Exception as e:
            self.state = AgentState.ERROR
            logger.error(f"Chat failed: {e}")
            raise
    
    async def _retrieve_context(self, query: str) -> List:
        """–ù–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ HybridRetriever."""
        try:
            limit = self.config.get("retrieval_limit", 5)
            start = time.perf_counter()

            context_results: List = []

            # L0: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 15 —Å—ã—Ä—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ Redis
            if self.memory and self.memory.redis_store:
                try:
                    l0_items = await self.memory.redis_store.l0_get_recent(15)
                    from src.core.retrieval import RetrievalResult
                    for item in l0_items:
                        context_results.append(
                            RetrievalResult(
                                content=item.get("content", ""),
                                score=item.get("importance", 0.5),
                                source="l0_raw",
                                metadata={"timestamp": item.get("timestamp")},
                                episode_id=item.get("stream_id"),
                            )
                        )
                except Exception as e:
                    logger.warning(f"L0 context fetch failed: {e}")

                # L1: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 —Å–∞–º–º–∞—Ä–∏
                try:
                    l1_summaries = await self.memory.redis_store.l1_get_recent_summaries(3)
                    from src.core.retrieval import RetrievalResult
                    for item in l1_summaries:
                        context_results.append(
                            RetrievalResult(
                                content=item.get("summary", ""),
                                score=item.get("importance", 0.6),
                                source="l1_summary",
                                metadata={"created_at": item.get("created_at"), "session_id": item.get("session_id")},
                                episode_id=item.get("session_id"),
                            )
                        )
                except Exception as e:
                    logger.warning(f"L1 summary fetch failed: {e}")

            # –†–µ—à–∞–µ–º –ª–∏–º–∏—Ç –¥–ª—è L2 –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞–ª–∏—á–∏—è L0/L1
            l2_limit = limit
            if len(context_results) >= 3:
                l2_limit = max(1, limit - 2)

            # L2: Graphiti —á–µ—Ä–µ–∑ HybridRetriever
            graph_results = await self.retriever.search(query, limit=l2_limit)
            context_results.extend(graph_results)

            duration = time.perf_counter() - start
            try:
                retrieval_latency.labels("hybrid_search").observe(duration)
            except Exception:
                pass
            logger.debug(f"Retrieved context: l0/l1={len(context_results)-len(graph_results)}, l2={len(graph_results)}")
            return context_results
        except Exception as e:
            logger.warning(f"Context retrieval failed: {e}")
            return []
    
    async def _get_strategies(self, query: str) -> List:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–∑ ReasoningBank."""
        try:
            if not self.reasoning:
                logger.debug("ReasoningBank not available")
                return []
            task_type = self._classify_task(query)
            logger.debug(f"Getting strategies for task_type={task_type}, query={query[:50]}")
            strategies = await self.reasoning.get_strategies(
                task_type=task_type,
                limit=3,
            )
            logger.info(f"Found {len(strategies)} strategies for task_type={task_type}")
            if strategies:
                for s in strategies:
                    logger.debug(f"  - {s.description[:50]}... (confidence: {s.success_rate or 0.0:.2f})")
            return strategies
        except Exception as e:
            logger.warning(f"Strategy retrieval failed: {e}", exc_info=True)
            return []
    
    def _classify_task(self, message: str) -> str:
        """–ü—Ä–æ—Å—Ç–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏."""
        message_lower = message.lower()
        
        if any(word in message_lower for word in ["–∫–æ–¥", "python", "—Ñ—É–Ω–∫—Ü–∏", "–ø—Ä–æ–≥—Ä–∞–º–º"]):
            return "coding"
        elif any(word in message_lower for word in ["–Ω–∞–ø–∏—à–∏", "—Å–æ–∑–¥–∞–π", "—Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π"]):
            return "generation"
        elif any(word in message_lower for word in ["–æ–±—ä—è—Å–Ω–∏", "–ø–æ—á–µ–º—É", "–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç"]):
            return "explanation"
        elif any(word in message_lower for word in ["–Ω–∞–π–¥–∏", "–ø–æ–∏—Å–∫", "–≥–¥–µ"]):
            return "search"
        elif any(word in message_lower for word in ["–ø–æ–º–Ω–∏—à—å", "–≥–æ–≤–æ—Ä–∏–ª", "—Ä–∞–Ω—å—à–µ"]):
            return "memory_recall"
        else:
            return "general"
    
    async def _generate_response(
        self,
        message: str,
        context: List,
        strategies: List,
    ) -> str:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é LLM."""
        if not self.llm_client:
            logger.warning("LLM client not available, using fallback")
            return self._fallback_response(message, context)
        
        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        system_prompt = self._build_system_prompt()
        
        # –î–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø–∞–º—è—Ç–∏ (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä)
        if context:
            context_items = []
            total_context_chars = 0
            max_context_chars = 3000  # ~750 —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
            for c in context[:5]:
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                content = c.content[:400] if len(c.content) > 400 else c.content  # –ë—ã–ª–æ –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
                context_item = f"[–ü–∞–º—è—Ç—å, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å {c.score:.2f}]: {content}"
                if total_context_chars + len(context_item) > max_context_chars:
                    break
                context_items.append(context_item)
                total_context_chars += len(context_item)
            
            if context_items:
                context_text = "\n\n".join(context_items)
                system_prompt += f"\n\n–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø–∞–º—è—Ç–∏:\n{context_text}"
        
        # –î–æ–±–∞–≤–∏—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å confidence –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        if strategies:
            strategies_text = "\n".join([
                f"- {s.description or 'Unknown'} (Confidence: {s.success_rate or 0.0:.2f}, Used: {s.usage_count or 0}x)"
                for s in strategies[:3]
            ])
            best_strategy = strategies[0] if strategies else None
            if best_strategy:
                desc = best_strategy.description or "Unknown strategy"
                conf = best_strategy.success_rate or 0.0
                usage = best_strategy.usage_count or 0
                system_prompt += f"\n\nüéØ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–ê–Ø –°–¢–†–ê–¢–ï–ì–ò–Ø: {desc}"
                system_prompt += f"\n   Confidence: {conf:.2f} | Usage: {usage} —Ä–∞–∑"
                logger.info(f"üìå Using strategy in prompt: {desc} (confidence: {conf:.2f})")
            system_prompt += f"\n\nüìö –î—Ä—É–≥–∏–µ —É—Å–ø–µ—à–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:\n{strategies_text}"
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä system_prompt (gpt-5-nano –∏–º–µ–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)
        # –ü—Ä–∏–º–µ—Ä–Ω–æ 1 —Ç–æ–∫–µ–Ω = 4 —Å–∏–º–≤–æ–ª–∞, –ª–∏–º–∏—Ç –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è gpt-5-nano ~8000 —Ç–æ–∫–µ–Ω–æ–≤
        # –û—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å –¥–ª—è reasoning tokens (200-400) –∏ completion (5000)
        max_system_prompt_chars = 20000  # ~5000 —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è system prompt
        if len(system_prompt) > max_system_prompt_chars:
            logger.warning(f"System prompt too long ({len(system_prompt)} chars), truncating to {max_system_prompt_chars}")
            system_prompt = system_prompt[:max_system_prompt_chars] + "... [truncated]"
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π)
        logger.info(f"üõ†Ô∏è FINAL SYSTEM PROMPT (snippet): {system_prompt[-500:]}")
        estimated_system_tokens = int(len(system_prompt) / 2.5)  # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        logger.info(f"üìè System prompt size: {len(system_prompt)} chars (~{estimated_system_tokens} tokens)")
        
        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è LLM
        messages = [{"role": "system", "content": system_prompt}]
        
        # –î–æ–±–∞–≤–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å –ª–∏–º–∏—Ç)
        # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 —Å–æ–æ–±—â–µ–Ω–∏—è (–±—ã–ª–æ 5) —á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –º–µ—Å—Ç–∞ –¥–ª—è reasoning
        recent_history = self.conversation_history[-3:]
        total_history_chars = 0
        max_history_chars = 2000  # ~500 —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏
        
        for msg in recent_history:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–∞–∂–¥–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
            content = msg.content[:300] if len(msg.content) > 300 else msg.content  # –ë—ã–ª–æ 500
            if total_history_chars + len(content) > max_history_chars:
                break  # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º –¥–æ–±–∞–≤–ª—è—Ç—å, –µ—Å–ª–∏ –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –ª–∏–º–∏—Ç
            messages.append({
                "role": msg.role,
                "content": content,
            })
            total_history_chars += len(content)
        
        # –¢–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É)
        user_message = message[:1000] if len(message) > 1000 else message  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if not recent_history or recent_history[-1].content != message:
            messages.append({"role": "user", "content": user_message})
        
        # –õ–æ–≥–∏—Ä—É–µ–º –æ–±—â–∏–π —Ä–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞ (–¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ ~2.5 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω)
        total_prompt_chars = sum(len(m.get("content", "")) for m in messages)
        estimated_prompt_tokens = int(total_prompt_chars / 2.5)  # –ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
        logger.info(f"üìä Total prompt size: {total_prompt_chars} chars (~{estimated_prompt_tokens} tokens), messages: {len(messages)}")
        
        # --- DIAGNOSTIC START: PROMPT X-RAY ---
        print("\nüîç === PROMPT X-RAY (What LLM sees) ===")
        print(f"Total messages: {len(messages)}")
        for i, msg in enumerate(messages):
            role = msg.get("role", "UNKNOWN").upper()
            content = msg.get("content", "") or ""
            preview = content[:300] + "..." if len(content) > 300 else content
            print(f"[{i}] {role}: {preview}")
        print("üîç =====================================\n")
        # --- DIAGNOSTIC END ---
        
        try:
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è gpt-5-nano-2025-08-07 (–Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç temperature, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç max_completion_tokens)
            model = self.config.get("model", "gpt-5-nano-2025-08-07")
            
            logger.debug(f"Calling LLM with model={model}, messages_count={len(messages)}")
            
            # gpt-5-nano –∏—Å–ø–æ–ª—å–∑—É–µ—Ç reasoning tokens, –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –º–µ—Å—Ç–∞
            # reasoning –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 200-400 —Ç–æ–∫–µ–Ω–æ–≤
            # –í–ê–ñ–ù–û: gpt-5-nano –∏–º–µ–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ (~8000-16000 —Ç–æ–∫–µ–Ω–æ–≤)
            # –ï—Å–ª–∏ –ø—Ä–æ–º–ø—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω–µ—Ç –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É —Å finish_reason: length
            # –ü–æ—ç—Ç–æ–º—É –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º max_completion_tokens –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –ø—Ä–æ–º–ø—Ç–∞
            base_max_tokens = self.config.get("max_tokens", 5000)
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö (–ø—Ä–∏–º–µ—Ä–Ω–æ 2.5 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)
            estimated_prompt_tokens = int(sum(len(m.get("content", "")) for m in messages) / 2.5)
            
            # –ï—Å–ª–∏ –ø—Ä–æ–º–ø—Ç —É–∂–µ –±–æ–ª—å—à–æ–π, —É–º–µ–Ω—å—à–∞–µ–º max_completion_tokens
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –æ–±—â–∏–π –ª–∏–º–∏—Ç –º–æ–¥–µ–ª–∏ ~16000 —Ç–æ–∫–µ–Ω–æ–≤ (–∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞)
            # –û—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å –¥–ª—è reasoning tokens (400) –∏ overhead (500)
            max_total_tokens = 16000  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è gpt-5-nano
            available_for_completion = max_total_tokens - estimated_prompt_tokens - 400 - 500
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º—É–º –∏–∑ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω–æ–≥–æ –∏ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ
            max_tokens = min(base_max_tokens, max(500, available_for_completion))  # –ú–∏–Ω–∏–º—É–º 500 —Ç–æ–∫–µ–Ω–æ–≤
            
            logger.info(f"üìä Prompt tokens (estimated): ~{estimated_prompt_tokens:.0f}, Available for completion: ~{available_for_completion:.0f}, Using max_completion_tokens: {max_tokens}")
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ (—Ç–∞–π–º–∞—É—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ asyncio)
            # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 60 —Å–µ–∫—É–Ω–¥, —Ç.–∫. Graphiti –º–æ–∂–µ—Ç –∑–∞–Ω–∏–º–∞—Ç—å –≤—Ä–µ–º—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏
            # –ù–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–ø–µ—Ä—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ, —Ç–∞–∫ —á—Ç–æ –æ—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±—ã—Å—Ç—Ä–µ–µ
            llm_timeout = self.config.get("llm_timeout_seconds", 60.0)  # 60 —Å–µ–∫—É–Ω–¥ –æ–±—â–∏–π —Ç–∞–π–º–∞—É—Ç
            
            async def _call_llm():
                if self.llm_rate_limiter:
                    async with rate_limit(self.llm_rate_limiter):
                        return await self.llm_client.chat.completions.create(
                            model=model,
                            messages=messages,
                            max_completion_tokens=max_tokens,
                        )
                else:
                    return await self.llm_client.chat.completions.create(
                        model=model,
                        messages=messages,
                        max_completion_tokens=max_tokens,
                    )
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º —Å —Ç–∞–π–º–∞—É—Ç–æ–º –Ω–∞ —É—Ä–æ–≤–Ω–µ asyncio
            try:
                response = await asyncio.wait_for(_call_llm(), timeout=llm_timeout)
            except asyncio.TimeoutError:
                logger.error(f"LLM request timed out after {llm_timeout}s. Model: {model}")
                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –∑–∞–ø—Ä–æ—Å –∑–∞–Ω—è–ª —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –ø–æ–ø—ã—Ç–∫—É –ø–æ–∑–∂–µ."
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –æ—Ç–≤–µ—Ç–∞
            logger.debug(f"LLM API response: {type(response)}, choices count: {len(response.choices) if response.choices else 0}")
            
            if not response.choices:
                logger.error(f"LLM returned no choices. Full response: {response}")
                return self._fallback_response(message, context)
            
            result = response.choices[0].message.content
            logger.info(f"LLM response received: type={type(result)}, len={len(result) if result else 0}, content={repr(result[:100]) if result else 'None'}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç
            if result is None:
                logger.warning(f"LLM returned None. Model: {model}, Messages: {len(messages)}, Response object: {response}")
                return self._fallback_response(message, context)

            usage = getattr(response, "usage", None)
            total_tokens = self._usage_stat(usage, "total_tokens")
            prompt_tokens = self._usage_stat(usage, "prompt_tokens")
            completion_tokens = self._usage_stat(usage, "completion_tokens")
            if total_tokens:
                tokens_used.labels(component="llm_total").inc(total_tokens)
                tokens_per_query.observe(total_tokens)
            if prompt_tokens:
                tokens_used.labels(component="llm_prompt").inc(prompt_tokens)
            if completion_tokens:
                tokens_used.labels(component="llm_completion").inc(completion_tokens)
            
            if not result or not result.strip():
                logger.warning(f"LLM returned empty string. Model: {model}, Messages: {len(messages)}, Raw content: {repr(result)}")
                # –ü–æ–ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å finish_reason –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                finish_reason = getattr(response.choices[0], 'finish_reason', None)
                logger.warning(f"Finish reason: {finish_reason}")
                
                # –ï—Å–ª–∏ finish_reason = "length", –∑–Ω–∞—á–∏—Ç –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–ª–∞ –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
                # –ü–æ–ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑ —Å –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–º –ø—Ä–æ–º–ø—Ç–æ–º
                if finish_reason == "length":
                    logger.warning("LLM hit token limit, trying with shorter prompt")
                    try:
                        simple_messages = [
                            {"role": "system", "content": f"–¢—ã ‚Äî {self.agent_name}. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É."},
                            {"role": "user", "content": message[:500]}  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Å–æ–æ–±—â–µ–Ω–∏—è
                        ]
                        retry_response = await asyncio.wait_for(
                            self.llm_client.chat.completions.create(
                                model=model,
                                messages=simple_messages,
                                max_completion_tokens=500,  # –ú–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                            ),
                            timeout=20.0
                        )
                        if retry_response.choices and retry_response.choices[0].message.content:
                            retry_result = retry_response.choices[0].message.content.strip()
                            if retry_result:
                                logger.info("Retry with shorter prompt succeeded")
                                return retry_result
                            else:
                                logger.warning("Retry returned empty string again")
                        else:
                            logger.warning("Retry response has no choices or content")
                    except Exception as retry_error:
                        logger.warning(f"Retry failed: {retry_error}", exc_info=True)
                
                # –ï—Å–ª–∏ retry –Ω–µ –ø–æ–º–æ–≥, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–Ω—è—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
                return f"–°–µ—Ä–≥–µ–π, —è –ø–æ–Ω—è–ª –≤–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: '{message[:100]}...'. –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —Å–µ–π—á–∞—Å —É –º–µ–Ω—è –≤–æ–∑–Ω–∏–∫–ª–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ—Ç–≤–µ—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."
            
            return result
            
        except asyncio.TimeoutError:
            # –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤—ã—à–µ, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
            logger.error(f"LLM request timed out. Model: {self.config.get('model', 'gpt-5-nano-2025-08-07')}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –∑–∞–ø—Ä–æ—Å –∑–∞–Ω—è–ª —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –ø–æ–ø—ã—Ç–∫—É –ø–æ–∑–∂–µ."
        except Exception as e:
            logger.error(f"LLM generation failed: {e}", exc_info=True)
            error_msg = str(e)
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –æ—à–∏–±–æ–∫
            if "401" in error_msg or "authentication" in error_msg.lower():
                return "–û—à–∏–±–∫–∞: –ù–µ–≤–µ—Ä–Ω—ã–π API –∫–ª—é—á OpenAI. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ OPENAI_API_KEY."
            if "model" in error_msg.lower() and "not found" in error_msg.lower():
                return f"–û—à–∏–±–∫–∞: –ú–æ–¥–µ–ª—å {self.config.get('model', 'gpt-5-nano-2025-08-07')} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏."
            if "timeout" in error_msg.lower() or "timed out" in error_msg.lower():
                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –∑–∞–ø—Ä–æ—Å –∑–∞–Ω—è–ª —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –ø–æ–ø—ã—Ç–∫—É –ø–æ–∑–∂–µ."
            if "rate limit" in error_msg.lower() or "429" in error_msg:
                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ –Ω–µ–º–Ω–æ–≥–æ –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ max_tokens
            if "max_tokens" in error_msg.lower() or "output limit" in error_msg.lower() or "400" in error_msg:
                logger.warning("LLM hit max_tokens limit, trying with shorter prompt and higher limit")
                # –ü–æ–ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑ —Å —É–ø—Ä–æ—â–µ–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º –∏ —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –ª–∏–º–∏—Ç–æ–º
                try:
                    simple_messages = [
                        {"role": "system", "content": f"–¢—ã ‚Äî {self.agent_name}. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É."},
                        {"role": "user", "content": message[:500]}  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Å–æ–æ–±—â–µ–Ω–∏—è
                    ]
                    retry_response = await asyncio.wait_for(
                        self.llm_client.chat.completions.create(
                            model=model,
                            messages=simple_messages,
                            max_completion_tokens=5000,  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –ª–∏–º–∏—Ç –¥–ª—è retry
                        ),
                        timeout=30.0
                    )
                    if retry_response.choices and retry_response.choices[0].message.content:
                        retry_result = retry_response.choices[0].message.content.strip()
                        if retry_result:
                            logger.info("Retry with shorter prompt and higher max_tokens succeeded")
                            return retry_result
                except Exception as retry_error:
                    logger.warning(f"Retry failed: {retry_error}")
                return "–°–µ—Ä–≥–µ–π, –≤–∞—à –∑–∞–ø—Ä–æ—Å —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π –∏–ª–∏ –æ—Ç–≤–µ—Ç —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –º–µ—Å—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∫–æ—Ä–æ—á–µ –∏–ª–∏ —Ä–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞—Å—Ç–∏."
            return f"–û—à–∏–±–∫–∞ LLM: {error_msg[:200]}"

    @staticmethod
    def _usage_stat(usage: Optional[Any], field: str) -> Optional[int]:
        if usage is None:
            return None
        if hasattr(usage, field):
            return getattr(usage, field)
        if isinstance(usage, dict):
            return usage.get(field)
        return None

    def _fallback_response(self, message: str, context: List) -> str:
        """Fallback –æ—Ç–≤–µ—Ç –∫–æ–≥–¥–∞ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç."""
        # –ü—ã—Ç–∞–µ–º—Å—è –¥–∞—Ç—å –±–æ–ª–µ–µ –ø–æ–ª–µ–∑–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if context:
            context_text = context[0].content[:200] if context[0].content else ""
            return f"–°–µ—Ä–≥–µ–π, —è –Ω–∞—à–µ–ª –≤ –ø–∞–º—è—Ç–∏: {context_text}... –ù–æ —Å–µ–π—á–∞—Å —É –º–µ–Ω—è –≤–æ–∑–Ω–∏–∫–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ—Ç–≤–µ—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –ø–æ–ø—ã—Ç–∫—É."
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞, –ø—ã—Ç–∞–µ–º—Å—è –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        if self.conversation_history:
            last_user_msg = None
            for msg in reversed(self.conversation_history):
                if msg.role == "user":
                    last_user_msg = msg.content
                    break
            
            if last_user_msg:
                return f"–°–µ—Ä–≥–µ–π, —è –ø–æ–Ω—è–ª –≤–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: '{last_user_msg[:100]}...'. –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —Å–µ–π—á–∞—Å —É –º–µ–Ω—è –≤–æ–∑–Ω–∏–∫–ª–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ—Ç–≤–µ—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."
        
        return f"–°–µ—Ä–≥–µ–π, —è –ø–æ–ª—É—á–∏–ª –≤–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –Ω–æ —Å–µ–π—á–∞—Å —É –º–µ–Ω—è –≤–æ–∑–Ω–∏–∫–ª–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ—Ç–≤–µ—Ç–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."

    def _update_memory_metrics(self) -> None:
        if not self.memory:
            return
        try:
            memory_size.labels(level="l0").set(len(self.memory.l0_cache))
            memory_size.labels(level="l1").set(len(self.memory.l1_cache))
        except Exception:
            # –ü—Ä–æ–º–µ—Ç–µ–π –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω ‚Äî –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
            pass
    
    async def _save_to_memory(
        self,
        user_message: str,
        assistant_response: str,
        metadata: Optional[Dict] = None,
    ) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±–º–µ–Ω –≤ –ø–∞–º—è—Ç—å —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –≤–∞–∂–Ω–æ—Å—Ç–∏."""
        try:
            content = f"User: {user_message}\nAssistant: {assistant_response}"
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å –ø–∞–º—è—Ç—å
            if len(content) > 2000:
                content = content[:2000] + "..."
            
            # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
            importance = self._calculate_importance(user_message, assistant_response)
            
            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ø–∞–º—è—Ç—å —á–µ—Ä–µ–∑ remember() - —ç—Ç–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç –≤ L0, –∑–∞—Ç–µ–º –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –ø–µ—Ä–µ–Ω–µ—Å—ë—Ç –≤ L1/L2
            # –ù–ï —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–ø—Ä—è–º—É—é –≤ L2, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –¥–∞—Ç—å —Å–∏—Å—Ç–µ–º–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
            await self.memory.remember(
                content=content,
                importance=importance,
                metadata={
                    "type": "conversation",
                    "user_message": user_message[:200],
                    "timestamp": datetime.now().isoformat(),
                    **(metadata or {}),
                },
            )
            
            logger.info(f"Saved conversation with importance={importance:.2f} (will consolidate to L1/L2 automatically)")
        except Exception as e:
            # –ù–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –µ—Å–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å - –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º
            logger.warning(f"Failed to save to memory (non-critical): {e}")
    
    def _calculate_importance(self, user_message: str, assistant_response: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è."""
        importance = 0.5  # –ë–∞–∑–æ–≤–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å
        
        user_lower = user_message.lower()
        
        # –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è ‚Äî –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞
        personal_keywords = [
            "–º–µ–Ω—è –∑–æ–≤—É—Ç", "–º–æ–µ –∏–º—è", "—è ", "–º–æ—è ", "–º–æ–π ",
            "—Ç–µ–±—è –∑–æ–≤—É—Ç", "—Ç–≤–æ–µ –∏–º—è", "—Ç—ã ", "—Ç–≤–æ—è ", "—Ç–≤–æ–π ",
            "—Å–æ–∑–¥–∞—Ç–µ–ª—å", "—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫", "–∞–≤—Ç–æ—Ä",
        ]
        if any(kw in user_lower for kw in personal_keywords):
            importance = max(importance, 0.9)
        
        # –Ø–≤–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –∑–∞–ø–æ–º–Ω–∏—Ç—å
        remember_keywords = [
            "–∑–∞–ø–æ–º–Ω–∏", "—Å–æ—Ö—Ä–∞–Ω–∏", "–Ω–µ –∑–∞–±—É–¥—å", "–≤–∞–∂–Ω–æ",
            "–Ω–∞ –¥–æ–ª–≥–∏–π —Å—Ä–æ–∫", "–Ω–∞–≤—Å–µ–≥–¥–∞", "–ø–æ–º–Ω–∏",
        ]
        if any(kw in user_lower for kw in remember_keywords):
            importance = max(importance, 0.95)
        
        # –§–∞–∫—Ç—ã –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        fact_keywords = [
            "—ç—Ç–æ ", "—è–≤–ª—è–µ—Ç—Å—è", "–∑–Ω–∞—á–∏—Ç", "–æ–∑–Ω–∞—á–∞–µ—Ç",
            "–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è", "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "—Å—É—Ç—å",
        ]
        if any(kw in user_lower for kw in fact_keywords):
            importance = max(importance, 0.75)
        
        # –ü—Ä–æ–µ–∫—Ç—ã –∏ –∑–∞–¥–∞—á–∏
        project_keywords = [
            "–ø—Ä–æ–µ–∫—Ç", "–∑–∞–¥–∞—á–∞", "—Ü–µ–ª—å", "–ø–ª–∞–Ω",
            "—Ä–∞–±–æ—Ç–∞—é –Ω–∞–¥", "–¥–µ–ª–∞—é", "—Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—é",
        ]
        if any(kw in user_lower for kw in project_keywords):
            importance = max(importance, 0.8)
        
        return importance
    
    def _judge_outcome(self, next_user_message: Optional[str]) -> "Outcome":
        """
        –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π Judge: –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–∏–≥–≥–µ—Ä—ã -> FAIL, –ø–æ–∑–∏—Ç–∏–≤ -> SUCCESS (verified),
        –∏–Ω–∞—á–µ UNKNOWN (unverified).
        """
        from src.core.types import Outcome
        if not next_user_message:
            return Outcome.UNKNOWN
        text = next_user_message.lower()
        negative_triggers = ["wrong", "error", "bug", "fail", "no", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–æ—à–∏–±–∫–∞", "–Ω–µ–≤–µ—Ä–Ω–æ"]
        positive_triggers = ["ok", "thanks", "—Å–ø–∞—Å–∏–±–æ", "good", "great", "–¥–∞–ª—å—à–µ"]
        if any(t in text for t in negative_triggers):
            return Outcome.FAILURE
        if any(t in text for t in positive_triggers):
            return Outcome.SUCCESS
        return Outcome.UNKNOWN
    
    async def _save_to_l2_directly(
        self,
        content: str,
        importance: float,
        metadata: Optional[Dict] = None,
    ) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é –≤ L2 (Neo4j) –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
        try:
            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ L2 —á–µ—Ä–µ–∑ GraphitiStore (–Ω–æ–≤—ã–π API)
            if self.memory and self.memory.graphiti:
                await self.memory.graphiti.add_episode(
                    content=content,
                    importance=importance,
                    source="direct_save"
                )
                logger.info(f"Saved directly to L2 via GraphitiStore")
            else:
                logger.warning("GraphitiStore not available for direct save")
        except Exception as e:
            logger.warning(f"Failed to save directly to L2: {e}")
    
    async def _log_experience(
        self,
        message: str,
        response: str,
        context: List,
        next_user_message: Optional[str] = None,
    ) -> None:
        """
        –ó–∞–ø–∏—Å–∞—Ç—å –æ–ø—ã—Ç –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ª—É—á—à—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –±—ã–ª–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∞, –∏ –ª–æ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
        –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω—ã–º (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å, –¥–æ–±–∞–≤–∏–≤ –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞).
        """
        try:
            if not self.reasoning:
                return
            
            task_type = self._classify_task(message)
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é, –∫–æ—Ç–æ—Ä–∞—è –±—ã–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            strategies = await self._get_strategies(message)
            strategy_used = strategies[0].description if strategies and len(strategies) > 0 else None
            
            from src.core.types import Outcome
            outcome = self._judge_outcome(next_user_message)
            context_snapshot = response[:500]
            await self.reasoning.log_experience(
                task_type=task_type,
                query=message[:200],
                strategy_used=strategy_used or "default",
                outcome=outcome,
                feedback=f"Response length: {len(response)} chars, context items: {len(context)}",
                context_episode_id=getattr(self.memory, "last_episode_id", None) if self.memory else None,
                context_snapshot=context_snapshot,
            )
            logger.info(f"üìù Logged experience: task_type={task_type}, strategy={strategy_used}, outcome={outcome.value}")
        except Exception as e:
            # –ù–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –µ—Å–ª–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å - –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º
            logger.warning(f"Failed to log experience (non-critical): {e}")
    
    async def provide_feedback(
        self,
        positive: bool,
        message_index: int = -1,
    ) -> None:
        """
        –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –Ω–∞ –æ—Ç–≤–µ—Ç.
        
        Args:
            positive: True –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –±—ã–ª –ø–æ–ª–µ–∑–µ–Ω
            message_index: –ò–Ω–¥–µ–∫—Å —Å–æ–æ–±—â–µ–Ω–∏—è (-1 = –ø–æ—Å–ª–µ–¥–Ω–µ–µ)
        """
        try:
            if not self.reasoning:
                return
            # –ù–∞–π—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            if abs(message_index) > len(self.conversation_history):
                return
            
            msg = self.conversation_history[message_index]
            if msg.role != "assistant":
                # –ù–∞–π—Ç–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–π –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
                for i in range(message_index, -len(self.conversation_history) - 1, -1):
                    if self.conversation_history[i].role == "assistant":
                        msg = self.conversation_history[i]
                        break
            await self.reasoning.add_experience(
                context=f"response_preview={msg.content[:100]}",
                action="user_feedback",
                outcome=positive,
            )
            
            logger.info(f"Feedback recorded: {'positive' if positive else 'negative'}")
            
        except Exception as e:
            logger.warning(f"Failed to record feedback: {e}")
    
    async def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏ –∏ –∞–≥–µ–Ω—Ç–∞."""
        stats = {
            "state": self.state.value,
            "conversation_length": len(self.conversation_history),
            "initialized": self._initialized,
        }
        
        if self.memory and self._initialized:
            try:
                memory_stats = await self.memory.get_stats()
                stats["memory"] = memory_stats
            except Exception as e:
                stats["memory"] = {"error": str(e)}
        
        if self.reasoning and self._initialized:
            try:
                stats["strategies_count"] = len(self.reasoning.strategies)
                stats["experience_buffer"] = len(self.reasoning.experience_buffer)
            except Exception:
                pass
        
        self._update_memory_metrics()
        return stats
    
    async def load_file(self, filepath: str, source: Optional[str] = None) -> Dict:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç—å.
        
        Args:
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            source: –û–ø–∏—Å–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏
        """
        if not self._initialized:
            await self.initialize()
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –†–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞–Ω–∫–∏ –µ—Å–ª–∏ –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª
            chunks = self._split_into_chunks(content, max_size=2000)
            
            for i, chunk in enumerate(chunks):
                await self.memory.remember(
                    content=chunk,
                    importance=0.7,
                    metadata={
                        "type": "file",
                        "source": source or filepath,
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                    },
                )
            
            return {
                "success": True,
                "filepath": filepath,
                "chunks_created": len(chunks),
                "total_chars": len(content),
            }
            
        except Exception as e:
            logger.error(f"Failed to load file: {e}")
            return {
                "success": False,
                "error": str(e),
            }
    
    def _split_into_chunks(self, text: str, max_size: int = 2000) -> List[str]:
        """–†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏."""
        if len(text) <= max_size:
            return [text]
        
        chunks = []
        paragraphs = text.split('\n\n')
        current_chunk = ""
        
        for para in paragraphs:
            if len(current_chunk) + len(para) + 2 <= max_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para + "\n\n"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def clear_history(self) -> None:
        """–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ (–Ω–µ –ø–∞–º—è—Ç—å!)."""
        self.conversation_history = []
        logger.info("Conversation history cleared")
    
    async def close(self) -> None:
        """
        –ó–∞–∫—Ä—ã—Ç—å –≤—Å–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è.
        
        Note:
            –ó–∞–∫—Ä—ã–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç–æ–º (owned components).
            –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ –∏–∑–≤–Ω–µ, –Ω–µ –∑–∞–∫—Ä—ã–≤–∞—é—Ç—Å—è.
        """
        logger.info("Closing FractalAgent...")
        
        # Only close components we own
        if self._owns_memory and self.memory:
            await self.memory.close()
            logger.info("Closed owned FractalMemory")
        
        if self._owns_reasoning and self.reasoning:
            if hasattr(self.reasoning, 'close'):
                await self.reasoning.close()
                logger.info("Closed owned ReasoningBank")
        
        # Retriever typically doesn't need explicit close as it shares GraphitiStore
        # but we log for completeness
        if self._owns_retriever and self.retriever:
            logger.info("HybridRetriever cleanup (shares GraphitiStore, no explicit close needed)")
        
        self._initialized = False
        self.state = AgentState.IDLE
        logger.info("FractalAgent closed")


# Convenience —Ñ—É–Ω–∫—Ü–∏—è
async def create_agent(config: Optional[Dict] = None) -> FractalAgent:
    """–°–æ–∑–¥–∞—Ç—å –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–≥–µ–Ω—Ç–∞."""
    agent = FractalAgent(config)
    await agent.initialize()
    return agent



