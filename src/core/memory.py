"""
FractalMemory â€” Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ñ„Ñ€Ğ°ĞºÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸ĞµĞ¹.

Ğ£Ñ€Ğ¾Ğ²Ğ½Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸:
- L0: Working Memory (Python list, ÑĞµĞºÑƒĞ½Ğ´Ñ‹)
- L1: Short-Term Memory (Python dict, Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹-Ñ‡Ğ°ÑÑ‹)  
- L2: Medium-Term Memory (Graph, Ğ´Ğ½Ğ¸)
- L3: Long-Term Memory (Graph, Ğ¼ĞµÑÑÑ†Ñ‹-Ğ³Ğ¾Ğ´Ñ‹)

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:
    memory = FractalMemory(config)
    await memory.initialize()
    
    # Ğ—Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑŒ
    await memory.remember("ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ»ÑĞ±Ğ¸Ñ‚ Python")
    
    # Ğ’ÑĞ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑŒ
    results = await memory.recall("Ñ‡Ñ‚Ğ¾ Ğ»ÑĞ±Ğ¸Ñ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ")
    
    # ĞšĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ (Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ»Ğ¸ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ)
    await memory.consolidate()
"""

from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
import asyncio
import logging
import uuid
import numpy as np
import re
import json

from .graphiti_store import GraphitiStore, SearchResult
from .redis_store import RedisMemoryStore
from .embeddings import OpenAIEmbedder
from dataclasses import dataclass, field

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞœĞĞ”Ğ•Ğ›Ğ˜ Ğ”Ğ›Ğ¯ Ğ’ĞĞ£Ğ¢Ğ Ğ•ĞĞĞ•Ğ“Ğ Ğ˜Ğ¡ĞŸĞĞ›Ğ¬Ğ—ĞĞ’ĞĞĞ˜Ğ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class MemoryItem:
    """Ğ­Ğ»ĞµĞ¼ĞµĞ½Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ (Ğ´Ğ»Ñ L0/L1)"""
    id: str
    content: str
    embedding: Optional[np.ndarray] = None
    importance: float = 1.0
    access_count: int = 1
    created_at: datetime = field(default_factory=datetime.now)
    last_accessed: datetime = field(default_factory=datetime.now)
    level: int = 0  # 0=L0, 1=L1
    metadata: Dict = field(default_factory=dict)


@dataclass 
class ConsolidationResult:
    """Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸"""
    promoted: int  # ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ²Ñ‹ÑˆĞµ
    decayed: int   # ĞŸĞ¾Ğ½Ğ¸Ğ¶ĞµĞ½ importance
    deleted: int   # Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¾ (soft delete)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FRACTAL MEMORY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class FractalMemory:
    """
    Ğ¤Ñ€Ğ°ĞºÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸ĞµĞ¹ L0 â†’ L1 â†’ L2 â†’ L3.
    
    ĞŸÑ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹:
    1. Ğ’Ğ°Ğ¶Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑˆĞµ (ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ)
    2. ĞĞµĞ²Ğ°Ğ¶Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ (decay)
    3. Ğ’ÑÑ‘ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ
    """
    
    def __init__(self, config: Dict):
        """
        Args:
            config: {
                "neo4j_uri": "bolt://localhost:7687",
                "neo4j_user": "neo4j",
                "neo4j_password": "password",
                "redis_url": "redis://localhost:6379",
                "user_id": "default",
                "llm_client": <optional>,
                "embedding_func": <optional callable>,
                
                # ĞĞ¿Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
                "l0_capacity": 10,
                "l1_capacity": 50,
                "decay_rate_l0": 0.1,
                "decay_rate_l1": 0.05,
                "importance_threshold": 0.3,
                "consolidation_interval": 300,  # ÑĞµĞºÑƒĞ½Ğ´Ñ‹
            }
        """
        self.config = config
        self.user_id = config.get("user_id", "default")
        
        # Graphiti store (L2/L3) - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚
        self.graphiti: Optional[GraphitiStore] = None
        
        # Redis store Ğ´Ğ»Ñ L0/L1 (Ğ¿ĞµÑ€ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ) - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚
        self.redis_store: Optional[RedisMemoryStore] = None
        
        # L0: Working Memory (Ğ¾Ñ‡ĞµĞ½ÑŒ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ°Ñ)
        # In-memory ĞºÑÑˆ (ĞºĞ¾Ğ¿Ğ¸Ñ Ğ¸Ğ· Redis Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°)
        self.l0_cache: List[MemoryItem] = []
        self.l0_capacity = config.get("l0_capacity", 10)
        
        # L1: Short-Term Memory
        self.l1_cache: Dict[str, MemoryItem] = {}
        self.l1_capacity = config.get("l1_capacity", 50)
        
        # Decay rates
        self.decay_rate_l0 = config.get("decay_rate_l0", 0.1)
        self.decay_rate_l1 = config.get("decay_rate_l1", 0.05)
        self.importance_threshold = config.get("importance_threshold", 0.3)
        self.consolidation_interval = config.get("consolidation_interval", 300)
        # ĞĞ¾Ğ²Ñ‹Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸
        self.l0_consolidation_batch = config.get("l0_consolidation_batch", 15)
        self._llm_model = config.get("llm_model", "gpt-5-mini")
        self.last_episode_id: Optional[str] = None
        self._consolidation_lock = asyncio.Lock()
        self.auto_consolidate_l0 = config.get("auto_consolidate_l0", True)  # Ğ‘Ğ¾ĞµĞ²Ğ¾Ğ¹ default: True
        
        # Embedding function (FIX: Auto-init OpenAIEmbedder if not provided)
        self.embedding_func = config.get("embedding_func")
        if self.embedding_func is None:
            try:
                embedder = OpenAIEmbedder()
                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ĞµÑÑ‚ÑŒ Ğ»Ğ¸ ĞºĞ»ÑÑ‡, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ¿Ğ°Ğ´Ğ°Ñ‚ÑŒ ÑÑ€Ğ°Ğ·Ñƒ, ĞµÑĞ»Ğ¸ ĞµĞ³Ğ¾ Ğ½ĞµÑ‚
                if embedder.client:
                    self.embedding_func = embedder.get_embedding
                    logger.info("Using default OpenAIEmbedder for embeddings")
                else:
                    logger.warning("OPENAI_API_KEY not found, embeddings will be disabled")
            except Exception as e:
                logger.warning(f"Failed to init default embedder: {e}")
        
        # State
        self._initialized = False
        self._consolidation_task = None
    
    async def initialize(self) -> None:
        """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸"""
        if self._initialized:
            return
        
        # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Redis store (Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚)
        redis_url = self.config.get("redis_url", "redis://localhost:6379")
        max_l0_size = self.config.get("l0_max_size", 500)
        self.redis_store = RedisMemoryStore(redis_url, self.user_id, max_l0_size)
        await self.redis_store.connect()
        
        # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Graphiti store (Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚)
        # Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ: Ğ¿Ğ°Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ Ğ² ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğµ, Ğ±ĞµĞ· Ğ´ĞµÑ„Ğ¾Ğ»Ñ‚Ğ°
        neo4j_password = self.config.get("neo4j_password")
        if not neo4j_password:
            raise ValueError(
                "neo4j_password is required. Set it in config or NEO4J_PASSWORD environment variable."
            )
        
        self.graphiti = GraphitiStore(
            neo4j_uri=self.config.get("neo4j_uri", "bolt://localhost:7687"),
            neo4j_user=self.config.get("neo4j_user", "neo4j"),
            neo4j_password=neo4j_password,
            user_id=self.user_id
        )
        await self.graphiti.connect()
        
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ L0/L1 Ğ¸Ğ· Redis
        await self._load_from_redis()
        
        self._initialized = True
        logger.info(f"FractalMemory initialized for user {self.user_id}")
        
        if (
            self.consolidation_interval
            and self.consolidation_interval > 0
            and (self._consolidation_task is None or self._consolidation_task.done())
        ):
            self._consolidation_task = asyncio.create_task(
                self._consolidation_loop()
            )
    
    async def _load_from_redis(self) -> None:
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ L0/L1 Ğ¸Ğ· Redis Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ñ€Ñ‚Ğµ."""
        if not self.redis_store:
            return
        
        try:
            # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ L0 (Ğ½Ğ¾Ğ²Ñ‹Ğ¹ API: l0_get_recent)
            l0_items = await self.redis_store.l0_get_recent(count=100)
            self.l0_cache = []
            for item in l0_items:
                try:
                    created_at = datetime.fromisoformat(item.get("timestamp", datetime.now().isoformat()))
                except:
                    created_at = datetime.now()
                
                self.l0_cache.append(MemoryItem(
                    id=item.get("stream_id", self._generate_id()),
                    content=item.get("content", ""),
                    embedding=None,
                    importance=item.get("importance", 0.5),
                    created_at=created_at,
                    last_accessed=created_at,
                    level=0,
                    metadata=item.get("metadata", {}),
                ))
            
            # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ L1 (Ğ½Ğ¾Ğ²Ñ‹Ğ¹ API: l1_get_sessions)
            l1_sessions = await self.redis_store.l1_get_sessions()
            self.l1_cache = {}
            for session in l1_sessions:
                try:
                    created_at = datetime.fromisoformat(session.get("created_at", datetime.now().isoformat()))
                except:
                    created_at = datetime.now()
                
                # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ MemoryItem Ğ´Ğ»Ñ ÑĞµÑÑĞ¸Ğ¸
                self.l1_cache[session.get("session_id")] = MemoryItem(
                    id=session.get("session_id", self._generate_id()),
                    content=session.get("summary", ""),
                    embedding=None,
                    importance=session.get("importance", 0.5),
                    created_at=created_at,
                    last_accessed=created_at,
                    level=1,
                    metadata={"session_id": session.get("session_id"), "source_count": session.get("source_count", 0)},
                )
            
            logger.info(f"Loaded from Redis: L0={len(self.l0_cache)}, L1 sessions={len(self.l1_cache)}")
            
        except Exception as e:
            logger.warning(f"Failed to load from Redis: {e}")
    
    async def close(self) -> None:
        """Ğ—Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸"""
        if self._consolidation_task:
            self._consolidation_task.cancel()
            try:
                await self._consolidation_task
            except asyncio.CancelledError:
                pass
            self._consolidation_task = None
        if self.graphiti:
            await self.graphiti.close()
        if self.redis_store:
            await self.redis_store.close()
        self._initialized = False
    
    async def _consolidation_loop(self) -> None:
        """Ğ¤Ğ¾Ğ½Ğ¾Ğ²Ğ°Ñ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ L0 â†’ L1 â†’ L2"""
        try:
            while self._initialized:
                await asyncio.sleep(self.consolidation_interval)
                if not self._initialized:
                    break
                try:
                    await self.consolidate()
                except Exception as exc:
                    logger.warning(f"Auto consolidation failed: {exc}")
        except asyncio.CancelledError:
            logger.debug("Consolidation loop cancelled")
            raise
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ĞĞ¡ĞĞĞ’ĞĞ«Ğ• ĞœĞ•Ğ¢ĞĞ”Ğ«
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def remember(
        self, 
        content: str,
        importance: float = 1.0,
        metadata: Dict = None
    ) -> str:
        """
        Ğ—Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ.
        Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ² L0, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ²Ñ‹ÑˆĞµ.
        
        Args:
            content: Ğ§Ñ‚Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑŒ
            importance: ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ (0-1)
            metadata: Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        
        Returns:
            ID ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸
        """
        self._ensure_initialized()
        
        metadata = metadata or {}
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ embedding ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ
        embedding = None
        if self.embedding_func:
            embedding = await self._get_embedding(content)
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ MemoryItem
        item = MemoryItem(
            id=self._generate_id(),
            content=content,
            embedding=embedding,
            importance=importance,
            created_at=datetime.now(),
            last_accessed=datetime.now(),
            level=0,
            metadata=dict(metadata)
        )
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ² L0 (in-memory ĞºÑÑˆ)
        self.l0_cache.append(item)
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ² Redis (Ğ¿ĞµÑ€ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾) - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ API
        if self.redis_store:
            await self.redis_store.l0_add(
                content=content,
                importance=importance,
                metadata=metadata
            )
        
        logger.debug(f"Added to L0: {item.id[:8]}... (importance={importance:.2f})")
        
        # Ğ‘Ğ°Ñ‚Ñ‡-Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸: Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğ¹ lock Ğ² Redis, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¸
        if self.auto_consolidate_l0 and self.redis_store:
            unconsolidated = await self.redis_store.l0_get_unconsolidated(limit=self.l0_consolidation_batch)
            if len(unconsolidated) >= self.l0_consolidation_batch:
                lock_key = f"memory:{self.user_id}:consolidation_lock"
                try:
                    # SETNX Ñ TTL 60s â€” ĞµÑĞ»Ğ¸ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ğ¼
                    locked = await self.redis_store.client.set(lock_key, "1", nx=True, ex=60)
                except Exception as exc:
                    logger.warning(f"Redis lock check failed: {exc}")
                    locked = False
                if locked:
                    logger.info("ğŸš€ Triggering Auto-Consolidation (lock acquired)")
                    asyncio.create_task(self._consolidate_l0_to_l1_locked_wrapper(lock_key))
                else:
                    logger.info("âš ï¸ Consolidation skipped (lock already active)")
        
        # ĞĞ²Ñ‚Ğ¾ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ L1â†’L2 Ğ¿Ñ€Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        important_in_l1 = sum(
            1 for item in self.l1_cache.values()
            if item.importance >= 0.7
        )
        if important_in_l1 >= 5:  # 5+ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹
            logger.info(f"Auto-consolidating L1â†’L2: {important_in_l1} important items")
            await self._consolidate_l1_to_l2()
        
        return item.id
    
    async def recall(
        self, 
        query: str,
        limit: int = 5,
        levels: List[int] = None
    ) -> List[SearchResult]:
        """
        Ğ’ÑĞ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ.
        Ğ˜Ñ‰ĞµÑ‚ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.
        
        Args:
            query: Ğ§Ñ‚Ğ¾ Ğ¸ÑĞºĞ°Ñ‚ÑŒ
            limit: ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
            levels: ĞšĞ°ĞºĞ¸Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ¸ÑĞºĞ°Ñ‚ÑŒ [0,1,2,3] Ğ¸Ğ»Ğ¸ None Ğ´Ğ»Ñ Ğ²ÑĞµÑ…
        
        Returns:
            Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¾Ñ‚ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸
        """
        self._ensure_initialized()
        
        if levels is None:
            levels = [0, 1, 2, 3]
        
        all_results = []
        
        # L0: Ğ¿Ğ¾Ğ¸ÑĞº Ğ² working memory
        if 0 in levels:
            l0_results = self._search_l0(query)
            all_results.extend(l0_results)
        
        # L1: Ğ¿Ğ¾Ğ¸ÑĞº Ğ² short-term
        if 1 in levels:
            l1_results = self._search_l1(query)
            all_results.extend(l1_results)
        
        # L2/L3: Ğ¿Ğ¾Ğ¸ÑĞº Ğ² Ğ³Ñ€Ğ°Ñ„Ğµ Ñ‡ĞµÑ€ĞµĞ· GraphitiStore
        graph_results = []
        if 2 in levels or 3 in levels:
            if self.graphiti:
                graphiti_results = await self.graphiti.search(query, limit=limit * 2)
                # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ SearchResult Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ»Ñ recall
                graph_results = [
                    SearchResult(
                        content=r.content,
                        score=r.score,
                        source=r.source,
                        timestamp=datetime.now(),
                        metadata=r.metadata or {}
                    )
                    for r in graphiti_results
                ]
            all_results.extend(graph_results)
        
        # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾ relevance
        all_results.sort(key=lambda x: x.score, reverse=True)
        top_results = all_results[:limit]

        # ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ access_count Ğ´Ğ»Ñ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ…
        await self._update_access_counts(top_results)

        # ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ last_accessed Ğ² Ğ³Ñ€Ğ°Ñ„Ğµ Ğ´Ğ»Ñ L2/L3 Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
        if graph_results:
            try:
                await self._update_graph_last_accessed(graph_results)
            except Exception as e:
                logger.warning(f"Failed to update graph last_accessed: {e}")
        
        return top_results

    async def _update_graph_last_accessed(
        self,
        results: List[SearchResult],
    ) -> None:
        """ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ last_accessed Ğ´Ğ»Ñ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¾Ğ² Ğ² Ğ³Ñ€Ğ°Ñ„Ğµ.

        ĞĞ¿Ğ¸Ñ€Ğ°ĞµĞ¼ÑÑ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Graphiti Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ² metadata
        Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¾Ğ² (uuid/episode_uuid/episode_name).
        """
        # Ğ¡Ğ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ²ÑĞµ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¾Ğ²
        episode_ids = set()
        for r in results:
            meta = r.metadata or {}
            for key in ("episode_id", "episode_uuid", "uuid", "episode_name", "name"):
                value = meta.get(key)
                if isinstance(value, str) and value:
                    episode_ids.add(value)
            
            episode_uuids = meta.get("episode_uuids")
            if isinstance(episode_uuids, list):
                for uid in episode_uuids:
                    if isinstance(uid, str) and uid:
                        episode_ids.add(uid)
            elif isinstance(episode_uuids, str) and episode_uuids:
                episode_ids.add(episode_uuids)

        if not episode_ids:
            logger.debug("No episode IDs found to update last_accessed")
            return

        try:
            if self.graphiti:
                # Ğ’ Graphiti ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ñ‹ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¼ĞµÑ‚ĞºÑƒ Episodic Ğ¸ Ğ¿Ğ¾Ğ»Ğµ uuid.
                await self.graphiti.execute_cypher(
                    """
                    MATCH (ep:Episodic)
                    WHERE ep.uuid IN $ids
                    SET ep.valid_at = datetime()
                    """,
                    {"ids": list(episode_ids)},
                )
                logger.debug(f"Updated valid_at for {len(episode_ids)} episodic IDs")
        except Exception as exc:
            logger.warning(f"Failed to update graph last_accessed: {exc}")
    
    async def search(
        self,
        query: str,
        limit: int = 5,
        levels: List[int] = None
    ) -> List[SearchResult]:
        """
        Search for information in memory (alias for recall).
        
        This method provides API compatibility with expected interface.
        
        Args:
            query: Search query
            limit: Maximum number of results
            levels: Which levels to search [0,1,2,3] or None for all
        
        Returns:
            List of results sorted by relevance
        """
        return await self.recall(query=query, limit=limit, levels=levels)
    
    async def consolidate(self) -> ConsolidationResult:
        """
        ĞšĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: L0 â†’ L1 â†’ L2 â†’ L3.
        Ğ’Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ»Ğ¸ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ.
        
        Returns:
            Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸
        """
        self._ensure_initialized()
        
        result = ConsolidationResult(promoted=0, decayed=0, deleted=0)
        
        # L0 â†’ L1
        r1 = await self._consolidate_l0_to_l1()
        result.promoted += r1.promoted
        result.decayed += r1.decayed
        result.deleted += r1.deleted
        
        # L1 â†’ L2
        r2 = await self._consolidate_l1_to_l2()
        result.promoted += r2.promoted
        result.decayed += r2.decayed
        result.deleted += r2.deleted
        
        # Apply decay to all levels
        await self._apply_decay()
        
        logger.info(
            f"Consolidation complete: "
            f"promoted={result.promoted}, "
            f"decayed={result.decayed}, "
            f"deleted={result.deleted}"
        )
        
        return result
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ĞšĞĞĞ¡ĞĞ›Ğ˜Ğ”ĞĞ¦Ğ˜Ğ¯
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def _consolidate_l0_to_l1(self) -> ConsolidationResult:
        """
        ĞšĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ L0 â†’ L1 Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ°Ñ‚Ñ‡-ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸, Ğ±ĞµĞ· Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ ÑĞ¿Ğ°Ğ¼Ğ° Ğ² Graphiti.
        Ğ¢Ñ€Ğ¸Ğ³Ğ³ĞµÑ€: Ğ¾Ğ±ÑŠÑ‘Ğ¼ L0 >= l0_consolidation_batch (15 Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ) Ğ¸Ğ»Ğ¸ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ².
        """
        result = ConsolidationResult(promoted=0, decayed=0, deleted=0)
        if not self.redis_store:
            return result
        
        # Ğ‘ĞµÑ€Ñ‘Ğ¼ Ğ½ĞµĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· Redis
        l0_items = await self.redis_store.l0_get_unconsolidated(limit=self.l0_consolidation_batch)
        if len(l0_items) < self.l0_consolidation_batch:
            # ĞĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ±Ğ°Ñ‚Ñ‡Ğ° â€” Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹
            return result
        
        # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ importance/Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ ÑˆÑƒĞ¼
        kept_items: List[MemoryItem] = []
        stream_ids: List[str] = []
        now = datetime.now()
        for raw in l0_items:
            created_at = datetime.fromisoformat(raw.get("timestamp")) if raw.get("timestamp") else now
            item = MemoryItem(
                id=raw.get("stream_id"),
                content=raw.get("content", ""),
                embedding=None,
                importance=float(raw.get("importance", 0.5)),
                created_at=created_at,
                last_accessed=created_at,
                level=0,
                metadata=raw.get("metadata", {}),
            )
            age_minutes = (now - item.created_at).total_seconds() / 60
            item.importance = self._calculate_importance(item, age_minutes)
            if item.importance < self.importance_threshold:
                result.deleted += 1
                continue
            kept_items.append(item)
            stream_ids.append(item.id)
        
        if not kept_items:
            await self.redis_store.l0_mark_consolidated(stream_ids)
            return result
        
        # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸ Ğ±Ğ°Ñ‚Ñ‡Ğ° Ñ‡ĞµÑ€ĞµĞ· GPT-5 Nano (fallback â€” concatenate)
        summary_text = await self._summarize_batch(kept_items)
        summary_id = self._generate_id()
        importance = max(i.importance for i in kept_items)
        summary_item = MemoryItem(
            id=summary_id,
            content=summary_text,
            importance=importance,
            created_at=datetime.now(),
            last_accessed=datetime.now(),
            level=1,
            metadata={"type": "conversation_summary", "source_ids": stream_ids},
        )
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² L1 (ĞºÑÑˆ + Redis hash + L1 summary list)
        self.l1_cache[summary_id] = summary_item
        await self.redis_store.l1_add_session(
            session_id=summary_id,
            summary=summary_text,
            importance=importance,
            source_ids=stream_ids,
        )
        await self.redis_store.l1_add_summary_entry(summary_id, summary_text, importance)
        
        # ĞĞ´Ğ½Ğ¸Ğ¼ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ¼ Ğ² Graphiti Ñ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ scale=meso
        if self.graphiti:
            try:
                episode_id = await self.graphiti.add_episode(
                    content=summary_text,
                    importance=importance,
                    source="l1_consolidation_summary",
                    metadata={"scale": "meso"},
                )
                self.last_episode_id = episode_id
            except Exception as exc:
                logger.warning(f"Failed to add summary to Graphiti: {exc}")
        
        # ĞŸĞ¾Ğ¼ĞµÑ‡Ğ°ĞµĞ¼ ĞºĞ°Ğº ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Redis
        await self.redis_store.l0_mark_consolidated(stream_ids)
        # ĞŸĞ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ L0 Ğ±ÑƒÑ„ĞµÑ€ Ğ² Redis Ğ¸ in-memory
        await self.redis_store.l0_clear_buffer()
        self.l0_cache.clear()
        
        result.promoted += 1
        return result

    async def _consolidate_l0_to_l1_locked(self) -> ConsolidationResult:
        """ĞĞ±ĞµÑ€Ñ‚ĞºĞ° Ñ Ğ»Ğ¾ĞºĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¾Ñ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ°Ñ‚Ñ‡ĞµĞ¹."""
        async with self._consolidation_lock:
            return await self._consolidate_l0_to_l1()
    
    async def _consolidate_l0_to_l1_locked_wrapper(self, lock_key: Optional[str] = None) -> ConsolidationResult:
        """ĞĞ±ĞµÑ€Ñ‚ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾ÑĞ²Ğ¾Ğ±Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ Redis-lock Ğ¿Ğ¾ÑĞ»Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ."""
        try:
            return await self._consolidate_l0_to_l1_locked()
        finally:
            if lock_key and self.redis_store and self.redis_store.client:
                try:
                    await self.redis_store.client.delete(lock_key)
                except Exception as exc:
                    logger.warning(f"Failed to release consolidation lock {lock_key}: {exc}")
    
    async def _consolidate_l1_to_l2(self) -> ConsolidationResult:
        """ĞšĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ L1 â†’ L2 (Ğ³Ñ€Ğ°Ñ„)"""
        result = ConsolidationResult(promoted=0, decayed=0, deleted=0)
        now = datetime.now()
        
        items_to_remove = []
        
        for item_id, item in list(self.l1_cache.items()):
            age_hours = (now - item.created_at).total_seconds() / 3600
            
            # Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ importance
            new_importance = self._calculate_importance(item, age_hours * 60)
            item.importance = new_importance
            
            # ĞšÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ»Ñ L2 (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğµ, Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ):
            # - ĞÑ‡ĞµĞ½ÑŒ Ğ²Ğ°Ğ¶Ğ½Ğ¾ (>= 0.85)
            # - Ğ˜Ğ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ (>= 5 Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹)
            # - Ğ˜Ğ»Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ñ‹
            should_promote = (
                new_importance >= 0.85 or 
                item.access_count >= 5 or
                self._contains_key_facts(item)
            )
            
            if should_promote:
                # Ğ¡Ğ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸ ÑƒĞ¶Ğµ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ² Graphiti Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ L0â†’L1
                if item.metadata.get("type") == "conversation_summary":
                    items_to_remove.append(item_id)
                    result.decayed += 1
                    continue
                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ´ÑƒĞ±Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼
                if not await self._is_duplicate_in_l2(item):
                    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ² Ğ³Ñ€Ğ°Ñ„
                    merged_metadata = dict(item.metadata or {})
                    merged_metadata["access_count"] = item.access_count
                    merged_metadata["user_id"] = self.user_id
                    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ² L2 Ñ‡ĞµÑ€ĞµĞ· GraphitiStore (Ğ½Ğ¾Ğ²Ñ‹Ğ¹ API)
                    if self.graphiti:
                        await self.graphiti.add_episode(
                            content=item.content,
                            importance=new_importance,
                            source="l1_consolidation"
                        )
                    
                    items_to_remove.append(item_id)
                    result.promoted += 1
                    logger.info(f"Promoted to L2: {item_id[:8]}... (importance={new_importance:.2f})")
                else:
                    logger.debug(f"Skipped duplicate for L2: {item_id[:8]}...")
                
            elif new_importance < self.importance_threshold and age_hours > 2:
                # Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ (Ğ·Ğ°Ğ±Ñ‹Ñ‚ÑŒ)
                items_to_remove.append(item_id)
                result.deleted += 1
                
            else:
                result.decayed += 1
        
        # Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ
        for item_id in items_to_remove:
            self.l1_cache.pop(item_id, None)
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ capacity
        if len(self.l1_cache) > self.l1_capacity:
            # Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ğ½Ğ°Ğ¸Ğ¼ĞµĞ½ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ
            sorted_items = sorted(
                self.l1_cache.items(),
                key=lambda x: x[1].importance
            )
            to_remove = len(self.l1_cache) - self.l1_capacity
            for item_id, _ in sorted_items[:to_remove]:
                self.l1_cache.pop(item_id, None)
                result.deleted += 1
        
        return result
    
    async def _apply_decay(self) -> None:
        """ĞŸÑ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ decay ĞºĞ¾ Ğ²ÑĞµĞ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼"""
        now = datetime.now()
        
        # L0 decay
        for item in self.l0_cache:
            age_minutes = (now - item.last_accessed).total_seconds() / 60
            item.importance *= np.exp(-self.decay_rate_l0 * age_minutes / 60)
        
        # L1 decay
        for item in self.l1_cache.values():
            age_hours = (now - item.last_accessed).total_seconds() / 3600
            item.importance *= np.exp(-self.decay_rate_l1 * age_hours)
        
        # L2/L3 decay Ğ² Ğ³Ñ€Ğ°Ñ„Ğµ (Ñ‡ĞµÑ€ĞµĞ· Cypher) â€” ÑƒĞ¿Ñ€Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾ Ğ´Ğ»Ñ Episodic.
        # Ğ’ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ ÑÑ…ĞµĞ¼Ğµ Graphiti Ğ½ĞµÑ‚ ÑĞ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ deleted/importance_score,
        # Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ·Ğ´ĞµÑÑŒ Ğ·Ğ°Ğ´ĞµĞ» Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ decay.
        if self.graphiti and self._initialized:
            try:
                await self.graphiti.execute_cypher(
                    """
                    MATCH (ep:Episodic)
                    RETURN count(ep) as cnt
                    """,
                    {},
                )
            except Exception as exc:
                logger.warning(f"Failed to apply graph decay: {exc}")
    
    def _calculate_importance(
        self, 
        item: MemoryItem, 
        age_minutes: float
    ) -> float:
        """
        Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ importance Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼:
        - Temporal decay
        - Access count (reinforcement)
        """
        # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ decay
        decay_rate = self.decay_rate_l0 if item.level == 0 else self.decay_rate_l1
        temporal_decay = np.exp(-decay_rate * age_minutes / 60)
        
        # Reinforcement: Ñ‡Ğ°ÑÑ‚Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğ·Ğ°Ğ¼ĞµĞ´Ğ»ÑĞµÑ‚ decay
        reinforcement = 1.0 + np.log1p(item.access_count) * 0.1
        
        # Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ importance
        new_importance = item.importance * temporal_decay * reinforcement
        
        return max(0.0, min(1.0, new_importance))
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ĞŸĞĞ˜Ğ¡Ğš
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _search_l0(self, query: str) -> List[SearchResult]:
        """ĞŸĞ¾Ğ¸ÑĞº Ğ² L0 (Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ keyword matching)"""
        results = []
        query_lower = query.lower()
        
        for item in self.l0_cache:
            if query_lower in item.content.lower():
                results.append(SearchResult(
                    content=item.content,
                    score=item.importance,
                    source="l0",
                    level="l0",
                    timestamp=item.created_at,
                    metadata={"level": 0}
                ))
        
        return results
    
    def _search_l1(self, query: str) -> List[SearchResult]:
        """ĞŸĞ¾Ğ¸ÑĞº Ğ² L1"""
        results = []
        query_lower = query.lower()
        
        for item in self.l1_cache.values():
            if query_lower in item.content.lower():
                results.append(SearchResult(
                    content=item.content,
                    score=item.importance,
                    source="l1",
                    level="l1",
                    timestamp=item.created_at,
                    metadata={"level": 1}
                ))
        
        return results
    
    async def _update_access_counts(self, results: List[SearchResult]) -> None:
        """ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑÑ‡Ñ‘Ñ‚Ñ‡Ğ¸ĞºĞ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²"""
        for result in results:
            level = result.metadata.get("level", 2)
            
            if level == 0:
                # L0
                for item in self.l0_cache:
                    if item.content == result.content:
                        item.access_count += 1
                        item.last_accessed = datetime.now()
                        break
                        
            elif level == 1:
                # L1
                for item in self.l1_cache.values():
                    if item.content == result.content:
                        item.access_count += 1
                        item.last_accessed = datetime.now()
                        break
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ’Ğ¡ĞŸĞĞœĞĞ“ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ• ĞœĞ•Ğ¢ĞĞ”Ğ«
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def _get_embedding(self, text: str) -> np.ndarray:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ embedding Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ°"""
        if self.embedding_func:
            return await self.embedding_func(text)
        return None
    
    def _generate_id(self) -> str:
        """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ID"""
        return str(uuid.uuid4())
    
    async def _summarize_batch(self, items: List[MemoryItem]) -> str:
        """
        LLM-ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ±Ğ°Ñ‚Ñ‡Ğ° (GPT-5 Mini). Ğ–Ñ‘ÑÑ‚ĞºĞ¸Ğ¹ JSON-Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ»Ğ¾Ğ³Ğ¸.
        """
        # Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ: Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ Ñ€Ğ¾Ğ»Ğ¸ Ğ¸ ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑÑ‹.
        lines: List[str] = []
        for it in items:
            raw = it.content or ""
            for line in raw.splitlines():
                line = line.strip()
                if not line:
                    continue
                role = "User"
                content = line
                low = line.lower()
                if low.startswith("user:"):
                    role = "User"
                    content = line[5:].strip()
                elif low.startswith("assistant:"):
                    role = "AI"
                    content = line[10:].strip()
                lines.append(f"{role}: {content}")
        text_block = "\n".join(lines)

        system_prompt = "You are a Data Processor. Output ONLY valid JSON."
        user_prompt = (
            f"DATA:\n{text_block}\n\n"
            "TASK: Summarize the data into a single 3rd-person paragraph.\n"
            "FORMAT: {\"summary\": \"The user discussed...\"}\n"
            "CONSTRAINT: Do not use timestamps or role labels in the summary value."
        )

        def _strip_code_blocks(text: str) -> str:
            t = text.strip() if text else ""
            if "```" in t:
                if "```json" in t:
                    t = t.split("```json", 1)[-1]
                t = t.split("```", 1)[-1]
            return t.strip()

        async def _call_llm() -> str:
            from openai import AsyncOpenAI  # type: ignore
            client = AsyncOpenAI()
            resp = await client.chat.completions.create(
                model=self._llm_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.0,
                max_completion_tokens=400,
            )
            if not resp.choices:
                return ""
            return resp.choices[0].message.content or ""

        try:
            raw_text = await _call_llm()
            cleaned = _strip_code_blocks(raw_text)
            try:
                data = json.loads(cleaned)
                summary = data.get("summary") if isinstance(data, dict) else None
                if summary and isinstance(summary, str):
                    return summary.strip()
            except Exception:
                pass
            if cleaned:
                return cleaned.strip()
        except Exception as exc:
            logger.warning(f"LLM summary failed, using fallback: {exc}")

        # Fallback: ÑĞºĞ»ĞµĞ¸Ğ²Ğ°ĞµĞ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ±ĞµĞ· Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ¾Ğ²
        payload = []
        for line in lines:
            payload.append(line.split(":", 1)[1].strip() if ":" in line else line.strip())
        return " ".join(payload)[:2000] or "Summary unavailable."
    
    def _ensure_initialized(self):
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ"""
        if not self._initialized:
            raise RuntimeError(
                "FractalMemory not initialized. "
                "Call await memory.initialize() first."
            )
    
    def _contains_key_facts(self, item: MemoryItem) -> bool:
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ»Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ñ‹."""
        key_patterns = [
            "Ğ¼ĞµĞ½Ñ Ğ·Ğ¾Ğ²ÑƒÑ‚", "Ğ¼Ğ¾Ğµ Ğ¸Ğ¼Ñ", "Ñ ",
            "Ñ‚ĞµĞ±Ñ Ğ·Ğ¾Ğ²ÑƒÑ‚", "Ñ‚Ğ²Ğ¾Ğµ Ğ¸Ğ¼Ñ", "Ñ‚Ñ‹ ",
            "Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸", "Ğ½Ğµ Ğ·Ğ°Ğ±ÑƒĞ´ÑŒ", "Ğ²Ğ°Ğ¶Ğ½Ğ¾",
            "ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ĞµĞ»ÑŒ", "Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº",
            "Ğ¿Ñ€Ğ¾ĞµĞºÑ‚", "Ñ†ĞµĞ»ÑŒ", "Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°",
        ]
        
        content = item.content.lower()
        return any(pattern in content for pattern in key_patterns)
    
    async def _is_duplicate_in_l2(self, item: MemoryItem) -> bool:
        """
        ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚, ĞµÑÑ‚ÑŒ Ğ»Ğ¸ Ğ² Graphiti (Episodic) ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´ ÑĞ¾ ÑÑ…Ğ¾Ğ¶Ğ¸Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼.
        
        Ğ¢Ğ°Ğº ĞºĞ°Ğº Ñƒ Ğ½Ğ°Ñ Ğ½ĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ñ user_id, Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ user-Ñ‚ĞµĞ³Ñƒ,
        ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ² content Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¾Ğ².
        """
        if not self.graphiti:
            return False
        
        snippet = item.content[:200].lower()
        user_tag = f"[user:{self.user_id}]"
        
        try:
            results = await self.graphiti.execute_cypher(
                """
                MATCH (ep:Episodic)
                WHERE ep.content CONTAINS $user_tag
                  AND toLower(ep.content) CONTAINS $snippet
                RETURN count(ep) AS cnt
                """,
                {"snippet": snippet, "user_tag": user_tag},
            )
            return bool(results and results[0].get("cnt", 0) > 0)
        except Exception as e:
            logger.warning(f"Duplicate check failed: {e}")
            return False
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def get_stats(self) -> Dict:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ (Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ L2/L3 Ğ¸Ğ· Neo4j Ğ¸ L0/L1 Ğ¸Ğ· Redis)"""
        # Ğ˜Ğ· Redis
        redis_stats = {}
        if self.redis_store:
            try:
                redis_stats = await self.redis_store.get_stats()
            except Exception as e:
                logger.warning(f"Failed to get Redis stats: {e}")
        
        stats = {
            "l0_size": redis_stats.get("l0_count", len(self.l0_cache)),
            "l0_capacity": self.l0_capacity,
            "l1_size": redis_stats.get("l1_count", len(self.l1_cache)),
            "l1_sessions": redis_stats.get("l1_sessions", 0),
            "l1_capacity": self.l1_capacity,
            "l0_avg_importance": float(np.mean([i.importance for i in self.l0_cache])) if self.l0_cache else 0.0,
            "l1_avg_importance": float(np.mean([i.importance for i in self.l1_cache.values()])) if self.l1_cache else 0.0,
            "l2_count": 0,
            "l3_count": 0,
            "user_id": self.user_id,
        }
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ¸Ğ· Graphiti (L2/L3)
        try:
            if self.graphiti and self._initialized:
                graph_stats = await self.graphiti.get_stats()
                stats["l2_count"] = graph_stats.get("l2_count", 0)
                stats["l3_count"] = graph_stats.get("l3_count", 0)
                stats["total_episodes"] = graph_stats.get("total_episodes", 0)
        except Exception as e:
            logger.warning(f"Failed to get Neo4j/Graphiti stats: {e}")
        
        return stats
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GARBAGE COLLECTION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def garbage_collect(
        self,
        dry_run: bool = False,
    ) -> Dict[str, int]:
        """
        Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ ÑĞ±Ğ¾Ñ€ĞºĞ° Ğ¼ÑƒÑĞ¾Ñ€Ğ° Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ retention policies.
        
        Ğ”ĞµĞ»ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ² GraphitiAdapter.safe_garbage_collect(), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹:
        - ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ retention period Ğ¿Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼ (L0/L1/L2/L3)
        - ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸ĞµĞ¼
        - ĞÑ€Ñ…Ğ¸Ğ²Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ´ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸ĞµĞ¼
        
        Args:
            dry_run: Ğ•ÑĞ»Ğ¸ True, Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‡Ñ‚Ğ¾ Ğ±ÑƒĞ´ĞµÑ‚ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¾
        
        Returns:
            Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° GC
        """
        self._ensure_initialized()
        
        # ĞÑ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ L0/L1 (in-memory)
        cutoff_time = datetime.now() - timedelta(hours=24)
        l0_original = len(self.l0_cache)
        self.l0_cache = [
            item
            for item in self.l0_cache
            if item.created_at > cutoff_time or item.importance > 0.5
        ]
        l0_cleaned = l0_original - len(self.l0_cache)
        
        l1_original = len(self.l1_cache)
        self.l1_cache = {
            item_id: item
            for item_id, item in self.l1_cache.items()
            if item.last_accessed > cutoff_time or item.importance > 0.3
        }
        l1_cleaned = l1_original - len(self.l1_cache)
        
        # GC Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ñ„Ğ° (L2/L3) Ñ‡ĞµÑ€ĞµĞ· GraphitiStore
        graph_stats = {"candidates": 0, "deleted": 0, "errors": []}
        if self.graphiti:
            try:
                retention_days = self.config.get("retention_days", 90)
                graph_stats = await self.graphiti.garbage_collect(
                    retention_days=retention_days,
                    dry_run=dry_run
                )
            except Exception as e:
                logger.warning(f"Graph GC failed: {e}")
                graph_stats["errors"].append(str(e))
        
        stats = {
            "l0_cleaned": l0_cleaned,
            "l1_cleaned": l1_cleaned,
            "graph_candidates": graph_stats.get("candidates", 0),
            "graph_deleted": graph_stats.get("deleted", 0),
            "graph_skipped_retention": graph_stats.get("skipped_retention", 0),
            "graph_skipped_links": graph_stats.get("skipped_links", 0),
            "dry_run": dry_run,
        }
        
        logger.info(
            f"GC {'(dry run) ' if dry_run else ''}"
            f"L0 cleaned={l0_cleaned}, L1 cleaned={l1_cleaned}, "
            f"graph deleted={graph_stats.get('deleted', 0)}"
        )
        return stats


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¤ĞĞ‘Ğ Ğ˜ĞšĞ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_fractal_memory(config: Dict) -> FractalMemory:
    """
    Ğ¤Ğ°Ğ±Ñ€Ğ¸ĞºĞ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ FractalMemory.
    
    Args:
        config: ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ (ÑĞ¼. FractalMemory.__init__)
    
    Returns:
        FractalMemory instance
    """
    return FractalMemory(config)

